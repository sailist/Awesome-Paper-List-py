1. A Copy-Augmented Generative Model for Open-Domain Question Answering
2. A Flexible Multi-Task Model for BERT Serving
3. A Recipe for Arbitrary Text Style Transfer with Large Language Models
4. A Risk-Averse Mechanism for Suicidality Assessment on Social Media
5. A Simple but Effective Pluggable Entity Lookup Table for Pre-trained Language Models
6. Adjusting the Precision-Recall Trade-Off with Align-and-Predict Decoding for Grammatical Error Correction
7. An Analysis of Negation in Natural Language Understanding Corpora
8. An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers
9. Analyzing Wrap-Up Effects through an Information-Theoretic Lens
10. Are Shortest Rationales the Best Explanations for Human Understanding?
11. As Little as Possible, as Much as Necessary: Detecting Over- and Undertranslations with Contrastive Conditioning
12. Augmenting Document Representations for Dense Retrieval with Interpolation and Perturbation
13. Automatic Detection of Entity-Manipulated Text using Factual Knowledge
14. BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models
15. Buy Tesla, Sell Ford: Assessing Implicit Stock Market Preference in Pre-trained Language Models
16. C-MORE: Pretraining to Answer Open-Domain Questions by Consulting Millions of References
17. Can a Transformer Pass the Wug Test? Tuning Copying Bias in Neural Morphological Inflection Models
18. Can Visual Dialogue Models Do Scorekeeping? Exploring How Dialogue Representations Incrementally Encode Shared Knowledge
19. Canary Extraction in Natural Language Understanding Models
20. CoDA21: Evaluating Language Understanding Capabilities of NLP Models With Context-Definition Alignment
21. Code Synonyms Do Matter: Multiple Synonyms Matching Network for Automatic ICD Coding
22. Complex Evolutional Pattern Learning for Temporal Knowledge Graph Reasoning
23. Contrastive Learning-Enhanced Nearest Neighbor Mechanism for Multi-Label Text Classification
24. Counterfactual Explanations for Natural Language Interfaces
25. Data Contamination: From Memorization to Exploitation
26. Detecting Annotation Errors in Morphological Data with the Transformer
27. Developmental Negation Processing in Transformer Language Models
28. Direct parsing to sentiment graphs
29. DiS-ReX: A Multilingual Dataset for Distantly Supervised Relation Extraction
30. Disentangled Knowledge Transfer for OOD Intent Discovery with Unified Contrastive Learning
31. DMix: Adaptive Distance-aware Interpolative Mixup
32. Does BERT Know that the IS-A Relation Is Transitive?
33. DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization
34. Efficient Classification of Long Documents Using Transformers
35. Estimating the Entropy of Linguistic Distributions
36. Event-Event Relation Extraction using Probabilistic Box Embedding
37. Exploiting Language Model Prompts Using Similarity Measures: A Case Study on the Word-in-Context Task
38. Fire Burns, Sword Cuts: Commonsense Inductive Bias for Exploration in Text-based Games
39. Focus on the Target’s Vocabulary: Masked Label Smoothing for Machine Translation
40. Have my arguments been replied to? Argument Pair Extraction as Machine Reading Comprehension
41. Hierarchical Curriculum Learning for AMR Parsing
42. High probability or low information? The probability–quality paradox in language generation
43. How Distributed are Distributed Representations? An Observation on the Locality of Syntactic Information in Verb Agreement Tasks
44. How does the pre-training objective affect what large language models learn about linguistic properties?
45. How reparametrization trick broke differentially-private text representation learning
46. HYPHEN: Hyperbolic Hawkes Attention For Text Streams
47. Investigating person-specific errors in chat-oriented dialogue systems
48. k-Rater Reliability: The Correct Unit of Reliability for Aggregated Human Annotations
49. Kronecker Decomposition for GPT Compression
50. Learning-by-Narrating: Narrative Pre-Training for Zero-Shot Dialogue Comprehension
51. Leveraging Explicit Lexico-logical Alignments in Text-to-SQL Parsing
52. LM-BFF-MS: Improving Few-Shot Fine-tuning of Language Models based on Multiple Soft Demonstration Memory
53. Machine Translation for Livonian: Catering to 20 Speakers
54. Mismatch between Multi-turn Dialogue and its Evaluation Metric in Dialogue State Tracking
55. Morphological Reinflection with Multiple Arguments: An Extended Annotation schema and a Georgian Case Study
56. Multilingual Pre-training with Language and Task Adaptation for Multilingual Text Style Transfer
57. NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better
58. On Efficiently Acquiring Annotations for Multilingual Models
59. On the Effect of Isotropy on VAE Representations of Text
60. On the Importance of Effectively Adapting Pretrained Language Models for Active Learning
61. On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations
62. P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks
63. PARE: A Simple and Strong Baseline for Monolingual and Multilingual Distantly Supervised Relation Extraction
64. Pixie: Preference in Implicit and Explicit Comparisons
65. Predicting Difficulty and Discrimination of Natural Language Questions
66. Predicting Sentence Deletions for Text Simplification Using a Functional Discourse Structure
67. PriMock57: A Dataset Of Primary Care Mock Consultations
68. Primum Non Nocere: Before working with Indigenous data, the ACL must confront ongoing colonialism
69. Probing the Robustness of Trained Metrics for Conversational Dialogue Systems
70. Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words
71. Region-dependent temperature scaling for certainty calibration and application to class-imbalanced token classification
72. Rethinking and Refining the Distinct Metric
73. Revisiting the Compositional Generalization Abilities of Neural Sequence Models
74. Rewarding Semantic Similarity under Optimized Alignments for AMR-to-Text Generation
75. S^4-Tuning: A Simple Cross-lingual Sub-network Tuning Method
76. Sample, Translate, Recombine: Leveraging Audio Alignments for Data Augmentation in End-to-end Speech Translation
77. SCD: Self-Contrastive Decorrelation of Sentence Embeddings
78. Sequence-to-sequence AMR Parsing with Ancestor Information
79. Simple and Effective Knowledge-Driven Query Expansion for QA-Based Product Attribute Extraction
80. Sub-Word Alignment is Still Useful: A Vest-Pocket Method for Enhancing Low-Resource Machine Translation
81. Text Smoothing: Enhance Various Data Augmentation Methods on Text Classification Tasks
82. The Power of Prompt Tuning for Low-Resource Semantic Parsing
83. To Find Waldo You Need Contextual Cues: Debiasing Who’s Waldo
84. Towards Consistent Document-level Entity Linking: Joint Models for Entity Linking and Coreference Resolution
85. Towards Fair Evaluation of Dialogue State Tracking by Flexible Incorporation of Turn-level Performances
86. Translate-Train Embracing Translationese Artifacts
87. Triangular Transfer: Freezing the Pivot for Triangular Machine Translation
88. (Un)solving Morphological Inflection: Lemma Overlap Artificially Inflates Models’ Performance
89. Understanding Game-Playing Agents with Natural Language Annotations
90. UniGDD: A Unified Generative Framework for Goal-Oriented Document-Grounded Dialogue
91. Unsupervised multiple-choice question generation for out-of-domain Q&A fine-tuning
92. Voxel-informed Language Grounding
93. When classifying grammatical role, BERT doesn't care about word order... except when it matters
94. When to Use Multi-Task Learning vs Intermediate Fine-Tuning for Pre-Trained Encoder Transfer Learning
95. WLASL-LEX: a Dataset for Recognising Phonological Properties in American Sign Language
96. XDBERT: Distilling Visual Information to BERT from Cross-Modal Systems to Improve Language Understanding
97. Zero-Shot Dependency Parsing with Worst-Case Aware Automated Curriculum Learning