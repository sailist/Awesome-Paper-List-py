<div data-v-fe7dd554="" data-v-1a17618c=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="title-admin el-col el-col-24"> Main Track Paper List </div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="primaryTitle el-row"><div data-v-fe7dd554="" class="el-col el-col-2"><img data-v-fe7dd554="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEQAAABECAYAAAA4E5OyAAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAARKADAAQAAAABAAAARAAAAADA2cO/AAAE+0lEQVR4Ae2aW6gVVRjH7WqZFgndKI+HA1EZaCUEXVF7KIwolBAhg3oMScIHMYoIfBMKeim6URBdKajwwYoQCiLzSdTEsocULLuZhlli9fvD/mDOOmvWrDUzezu7vT74s2bW+tZ3+e+ZdZs9bVqWzEBmIDPQfQauJsR3wEHwE/gI3AVGUlaR9XHwrwfPUncKGBm5lkxPAB8ZVrd6ZNgg0TcqyBAp348SId9EECJSxrtCyul9DmQ79o9G+BipcSSCj6ySGcgMZAYyA5mBzEBmIDOQGcgMhBkY1B7iVMLQIdEEmANmgNPAIbAJjNSO90YSPgDs/MMtD9M2UqdnbwfIMHJ+ROcs0Hm5jgjXgEsaRLqYvnoKLPmycryBj753nYWHD4AFf0VDjxo3Xi/YM7tWirBOPyGfOsGPcd+G3ISRr4ARYeUzLRjXBDGzBTtTTKygxgK1ct4UrfoVCvw+IGJ+Aa+COk+HZq7bwEvga3AMKN6/gWatV8C94AzQSIqvihFyc4XFB2jX+amSG4QswclOYPGFyr3orQL6IWrJz/RyHTwcsLSUtn96ff6i7Mtj2/OvtYu+5bjxxdy/T79ze3aSiiMeh++VWNDj+IOjv6hEt2n12RjwPb0xZJjOLmxcnBrIVjqYASv11Pgeubs9umsDDqfT9iC4NaBT1vQaDRZPk/JL7CSNWetLHF/jiVQDmhucplefjFO5DUhfBKfIQyi7fprcv5DifAbK+z0B+BLd4tHbQ50rd1LxK7AktrsKgft5tNkMYv2blhrz9Jk1WjRruE5lZL5jQdOdq3eCOg1+Jsu5sEFXukpO+5sY0Yc037rF9VnnfnNMAKajpbrPyYem0Cs/LtG7oKBXXKGKwOsLbVWXj6Hgi6ONOv1Il1YFYO0aicucarFj8jQXrt4+6rRoMhnn4kmwDBSfHG6DsoBWLbJc+23ePxKMoNAYIkTfau1XnuBaZxoW5B9cFwnjtpacT6/YhZf5rlO+GxvdOSiGHOh8Y6xnTMHfD+4Bc3p1TQr5/gKE/LfVtiMl0IMVQWmRc3mKwQhdrQ8+AW0lXGVHT3S0PI9mlcHD6KyIthhW1H7JN2tVxdCk/fdwSJNbb+c21tkmdBdP7h59pxntOVCcmmP9NtXTWxAt2qf8BlKcaiW6DtwAzgRlokFbS3i9Hlq3pPhoU1drnEkS+gfRcTS1mdKAGSsLURQkf4JvwZEeNIXOBZqZZoIuyNbUIHQa3uYv0jVbK1MJ0UJKs0nXEmkjHp3WTdn1FleUPrL0fj/ua/gf1L1MDsfq5KFzENu2t/HLdMGGVtcX1SHD+qRMwV1IuCqGNZZYk3ILnascDUO7zmJCs2s0R+5GbhiSd2PUmHhLdMYRitrAuU6G6f6JiByTVTYOKSk6zKqaVZPJUAe9f5+BYXoy9hPvhaBvor2IzhKGgRT91eKqvjFRMDyb60Ed5NQlXqtR92C8kEL7lzrd2gzqBtzPfvqiaBvN9jMPWNQ2/03Qz+RSbevr3GWBmAfSpG85xY9RqUm0pf8icUwfSMYRTrQ/eAu0lVyKnT34vSMixpOionOUvSAlobq6Ohd9FIRO6Gg++aJF0FKgUzctmesmXNZvNzZXg1lg6GSMiDeA70BZgjH1+nj1FFgCdCzRuvTFaEWUc2lfBLQ+uBJMAH3sOg9oFXy0B60uNS4YPudan0mzZAYyA5mBzjLwH3H7ah8z4OG+AAAAAElFTkSuQmCC" alt="" class="icon"></div><div data-v-fe7dd554="" class="el-col el-col-22"><p data-v-fe7dd554="" class="title">Oral Papers</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">42 <b>Hierarchical View Predictor: Unsupervised 3D Global Feature Learning through Hierarchical Prediction among Unordered Views<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhizhong Han; Xiyang Wang; Yu-Shen Liu*; Matthias Zwicker</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">66 <b>Multi-Perspective Video Captioning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yi Bin*; Xindi Shang; Bo Peng; Yujuan Ding; Tat-Seng Chua</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">87 <b>Towards robust cross-domain image understanding with unsupervised noise removal<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Lei Zhu; Zhaojing Luo*; Wei Wang; Meihui Zhang; Gang Chen; Kaiping Zheng</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">88 <b>DL-Easy: An Easy-to-Use Framework for MultiModal Analysis<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Naili Xing; Chris Yeung; Cheng-Hao Cai; Teck Khim Ng; Wei Wang; Kaiyuan Yang; Nan Yang; Meihui Zhang; Gang Chen; Beng Chin Ooi*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">99 <b>Self-supervised Multi-view Multi-Human Association and Tracking<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yiyang Gan; Ruize Han*; Liqiang Yin; Wei Feng; Song Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">114 <b>Semi-Autoregressive Image Captioning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xu Yan; Zhengcong Fei*; Zekang Li; Shuhui Wang; Qingming Huang; Qi Tian</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">119 <b>Dual Learning Music Composition and Dance Choreography<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Shuang Wu*; Zhenguang Liu; Shijian Lu; Li Cheng</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">147 <b>PUGCQ: A Large Scale Dataset for Quality Assessment of Professional User-Generated Content<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Guo Li; Baoliang Chen; Lingyu Zhu; Qinwen He; Hongfei Fan; Shiqi Wang*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">154 <b>Learning Hierarchal Channel Attention for Fine-grained Visual Classification<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xiang Guan; Guoqing Wang*; Xing Xu; Yi Bin</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">161 <b>DSP: Dual Soft-Paste for Unsupervised Domain Adaptive Semantic Segmentation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Li Gao; Jing Zhang; Lefei Zhang*; Dacheng Tao</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">167 <b>Efficient Multi-Modal Fusion with Diversity Analysis<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Shuhui Qu*; Yan Kang; Janghwan Lee</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">188 <b>Multi-Modal Sarcasm Detection with Interactive In-Modal and Cross-Modal Graphs<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Bin Liang*; Chenwei Lou; Xiang Li; Lin Gui; Min Yang; Ruifeng Xu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">196 <b> Multi-Source Fusion and Automatic Predictor Selection for Zero-Shot Video Object Segmentation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xiaoqi Zhao*; Youwei Pang; Jiaxing Yang; Lihe Zhang; Huchuan Lu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">197 <b>Progressive Graph Attention Network for Video Question Answering<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Liang Peng; Shuangji Yang; Yi Bin*; Guoqing Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">204 <b>MageAdd: Real-Time Interaction Simulation for Scene Synthesis<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Shao-Kui Zhang; Yi-Xiao Li; Yu He; Yongliang Yang; Song-Hai Zhang*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">208 <b>Video Background Music Generation with Controllable Music Transformer<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Shangzhe Di; Zeren Jiang; Si Liu*; Zhaokai Wang; Leyan Zhu; Zexin He; Hongming Liu; Shuicheng Yan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">235 <b>Robust Shadow Detection by Exploring Effective Shadow Contexts<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xianyong Fang*; Xiaohao He; Linbo Wang; Jianbing Shen</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">237 <b>Towards Cross-Granularity Few-Shot Learning: Coarse-to-Fine Pseudo-Labeling with Visual-Semantic Meta-Embedding<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jinhai Yang; Hua Yang*; Lin Chen</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">258 <b>One-Stage Incomplete Multi-view Clustering via Late Fusion<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yi Zhang*; Xinwang Liu; Siwei Wang; Jiyuan Liu; Sisi Dai; En Zhu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">261 <b>Mix-order Attention Networks for Image Restoration<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Tao Dai; Yalei Lv; Bin Chen*; Zhi Wang; Zexuan Zhu; Shu-Tao Xia</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">270 <b>GCCN: Geometric Constraint Co-attention Network for 6D Object Pose Estimation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yongming Wen; Yiquan Fang; Junhao Cai; Kimwa Tung; HUI CHENG*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">294 <b>Image Re-composition via Regional Content-Style Decoupling<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Rong Zhang; Wei Li; Yiqun Zhang; Hong Zhang; Jinhui Yu; Ruigang Yang; Weiwei Xu*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">298 <b>Enhanced Invertible Encoding for Learned Image Compression<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yueqi Xie; Ka Leong Cheng; Qifeng Chen*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">303 <b>Learning Spatial-angular Fusion for Compressive Light Field Imaging in a Cycle-consistent Framework<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xianqiang LYU; Zhiyu Zhu; Mantang Guo; Jing Jin; Junhui Hou*; Huanqiang Zeng</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">312 <b>Group-based Distinctive Image Captioning with Memory Attention<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jiuniu Wang*; Wenjia Xu; Qingzhong Wang; Antoni Chan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">320 <b>WeClick: Weakly-Supervised Video Semantic Segmentation with Click Annotations<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Peidong Liu*; Zibin He; Xiyu Yan; Yong Jiang; Shu-Tao Xia; Feng Zheng; Maowei Hu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">338 <b>SimulLR: Simultaneous Lip Reading Transducer with Attention-Guided Adaptive Memory<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhijie Lin*; Zhou Zhao; Haoyuan Li; Jinglin Liu; Meng Zhang; Xingshan Zeng; Xiaofei He</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">345 <b>Two-stage Visual Cues Enhancement Network for Referring Image Segmentation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yang Jiao; Zequn Jie; Weixin Luo; Jingjing Chen*; Yu-Gang Jiang; Xiaolin Wei; Lin Ma</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">348 <b>Speech2AffectiveGestures: Synthesizing Co-Speech Gestures with Generative Adversarial Affective Expression Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Uttaran Bhattacharya*; Elizabeth Childs; Nicholas S Rewkowski; Dinesh Manocha</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">353 <b>VQMG: Hierarchical Vector Quantised and Multi-hops Graph Reasoning for Explicit Representation Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Lei Li*; Chun Yuan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">381 <b>How to Learn a Domain-Adaptive Event Simulator?<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Daxin Gu; Jia Li*; Yu Zhang; Yonghong Tian</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">419 <b>Latent Memory-augmented Graph Transformer for Visual Storytelling<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Mengshi Qi*; Jie Qin; Di Huang; Zhiqiang Shen; Yi Yang; Jiebo Luo</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">430 <b>PIMNet: A Parallel, Iterative and Mimicking Network for Scene Text Recognition<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhi Qiao; Yu Zhou*; Jin Wei; Wei Wang; Yuan Zhang; Ning Jiang; Hongbin Wang; Weiping Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">437 <b>Fine-grained Cross-modal Alignment Network for Text-Video Retrieval<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Ning Han; Jingjing Chen; Guangyi Xiao; Hao Zhang; Yawen Zeng; Hao Chen*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">438 <b>DC-GNet: Deep Mesh Relation Capturing Graph Convolution Network for 3D Human Shape Reconstruction<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Shihao Zhou*; Mengxi Jiang; Shanshan Cai; Yunqi Lei</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">464 <b>Vehicle Counting Network with Attention-based Mask Refinement and Spatial-awareness Block Loss<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Ji Zhang; Jian-Jun Qiao; Xiao Wu*; Wei Li</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">474 <b>Cross-Modal Generalization: Learning in Low Resource Modalities via Meta-Alignment<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Paul Pu Liang*; Peter Wu; Liu Ziyin; Louis-Philippe Morency; Ruslan Salakhutdinov</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">510 <b>Hierarchical Multi-Task Learning for Diagram Question Answering with Multi-Modal Transformer<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhaoquan Yuan; Xiao Peng; Xiao Wu*; Changsheng Xu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">518 <b>Combining Attention with Flow for Person Image Synthesis<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yurui Ren; Yubo Wu; Thomas H Li; Shan Liu; Ge Li*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">525 <b>Mining Latent Structures for Multimedia Recommendation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jinghao Zhang; Yanqiao ZHU*; Qiang Liu; Shu Wu; Shuhui Wang; Liang Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">526 <b>Hierarchical Fusion for Practical Ghost-free High Dynamic Range Imaging<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Pengfei Xiong*; Yu Chen</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">551 <b>Video Visual Relation Detection via Iterative Inference<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xindi Shang*; Yicong Li; Junbin Xiao; Wei Ji; Tat-Seng Chua</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">558 <b>CausalRec: Causal Inference for Visual Debiasing in Visually-Aware Recommendation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Ruihong Qiu*; Sen Wang; Zhi Chen; Hongzhi Yin; Zi Huang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">563 <b>Deep Clustering based on Bi-Space Association Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Hao Huang*; Shinjae Yoo; Chenxiao Xu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">567 <b>Feature Stylization and Domain-aware Contrastive Learning for Domain Generalization<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Seogkyu Jeon*; Kibeom Hong ; Pilhyeon Lee; Jewook Lee; Hyeran Byun</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">574 <b>HDA-Net: Horizontal Deformable Attention Network for Stereo Matching<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Qi Zhang*; Xuesong Zhang; Baoping Li; Yuzhong Chen; Anlong Ming</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">578 <b>UniCon: Unified Context Network for Robust Active Speaker Detection<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yuanhang Zhang*; Susan Liang; Shuang Yang; Xiao Liu; Zhongqin Wu; Shiguang Shan; Xilin Chen</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">610 <b>CONQUER: Contextual Query-aware Ranking for Video Corpus Moment Retrieval<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhijian Hou*; Chong-Wah Ngo; W. K. Chan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">614 <b>Neighbor-view Enhanced Model for Vision and Language Navigation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Dong An; Yuankai Qi; Yan Huang*; Qi Wu; Liang Wang; Tieniu Tan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">685 <b>FoodLogoDet-1500: A Dataset for Large-Scale Food Logo Detection via Multi-Scale Feature Decoupling Network<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Qiang Hou; Weiqing Min; Jing Wang; Sujuan Hou*; Yuanjie Zheng; Shuqiang Jiang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">690 <b>Edge-oriented Convolution Block for Real-time Super Resolution on Mobile Devices<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xindong Zhang; Hui Zeng; Lei Zhang*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">708 <b>MS-GraphSIM: Inferring Point Cloud Quality via Multiscale Graph Similarity<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yujie Zhang*; Qi Yang; Yiling Xu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">709 <b>CaFGraph: Context-aware Facial Multi-graph Representation for Facial Action Unit Recognition<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yingjie Chen; Diqi Chen; Yizhou Wang; Tao Wang*; Yun Liang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">778 <b>Product-oriented Machine Translation with Cross-modal Cross-lingual Pre-training<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yuqing Song*; Shizhe Chen; Qin Jin; Wei Luo; Jun Xie; Fei Huang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">788 <b>Multiple Object Tracking by Trajectory Map Regression with Temporal Priors Embedding<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xingyu Wan*; Sanping Zhou; Jinjun Wang; Rongye Meng</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">813 <b>VoteHMR: Occlusion-Aware Voting Network for Robust 3D Human Mesh Recovery from Partial Point Clouds<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Guanze Liu; Yu Rong; Lu Sheng*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">839 <b>From Voxel to Point: IoU-guided 3D Object Detection for PointCloud with Voxel-to-Point Decoder<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jiale Li; Hang Dai*; Ling Shao; Yong Ding</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">864 <b>Ada-VSR: Adaptive Video Super-Resolution with Meta-Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Akash Gupta*; Padmaja Jonnalagedda; Bir Bhanu; Amit K. Roy-Chowdhury</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">897 <b>MBRS : Enhancing Robustness of DNN-based Watermarking by Mini-Batch of Real and Simulated JPEG Compression<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhaoyang Jia*; Han Fang; Weiming Zhang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">898 <b>QoE Ready to Respond: A QoE-aware MEC Selection Scheme for DASH-based Adaptive Video Streaming to Mobile Users<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Wanxin Shi; Qing Li*; Ruishan Zhang; Gengbiao Shen; Yong Jiang; Zhenhui Yuan; Gabriel-Miro Muntean</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">907 <b>Multimodal Global Relation Knowledge Distillation for Egocentric Action Anticipation <b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yi Huang; Xiaoshan Yang; Changsheng Xu*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">932 <b>From Synthetic to Real: Image Dehazing Collaborating with Unlabeled Real Data<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Ye Liu; Lei Zhu; Shunda Pei; Huazhu Fu; Jing Qin; Qing Zhang; Liang Wan*; Wei Feng</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">954 <b>Parametric Reshaping of Portraits in Videos<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xiangjun Tang; WenXin Sun; Yongliang Yang; Xiaogang Jin*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">968 <b>Instance-wise or Class-wise? A Tale of Neighbor Shapley for Concept-based Explanation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jiahui Li*; Kun Kuang; Lin Li; Long Chen; Songyang Zhang; Jian Shao; Jun Xiao</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">980 <b>Extending 6-DoF VR Experience Via Multi-Sphere Images Interpolation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jisheng Li*; Yuze He; Jinghui Jiao; Yubin Hu; Yuxing Han; Jiangtao Wen</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1059 <b>X-GGM: Graph Generative Modeling for Out-of-distribution Generalization in Visual Question Answering<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jingjing Jiang*; Ziyi Liu; Yifan Liu; Zhixiong Nan; Nanning Zheng</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1066 <b>Disentangle Your Dense Object Detector<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zehui Chen; Chenhongyi Yang; Qiaofei Li; Feng Zhao*; Zheng-Jun Zha; Feng Wu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1098 <b>Hybrid Network Compression via Meta-Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jianming Ye; Shiliang Zhang*; Jingdong Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1129 <b>Exploring Contextual-Aware Representation and Linguistic-Diverse Expression for Visual Dialog<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xiangpeng Li; Lianli Gao; Lei Zhao; Jingkuan Song*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1178 <b>Do We Really Need Frame-by-Frame Annotation Datasets for Object Tracking?<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Lei Hu; Shaoli Huang; Shilei Wang; Wei Liu; Jifeng Ning*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1192 <b>Identity-Preserving Face Anonymization via Adaptively Facial Attributes Obfuscation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jingzhi Li*; Lutong Han; Ruoyu Chen; Hua Zhang; Bing Han; Lili Wang; Xiaochun Cao</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1201 <b>DSSL: Deep Surroundings-person Separation Learning for Text-based Person Retrieval<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Aichun Zhu*; Zijie Wang; Yifeng Li; Xili Wan; Jing Jin; Tian Wang; Fangqiang Hu; Gang Hua</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1261 <b>Self-Representation Subspace Clustering for Incomplete Multi-view Data<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jiyuan Liu*; Xinwang Liu; Yi Zhang; Pei Zhang; Wenxuan Tu; Siwei Wang; Sihang Zhou; Weixuan Liang; Siqi Wang; Yuexiang Yang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1271 <b>Cross-Camera Feature Prediction for Intra-Camera Supervised Person Re-identification across Distant Scenes<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Wenhang Ge; Chunyan Pan; Ancong Wu*; Hongwei Zheng; WEI-SHI ZHENG</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1292 <b>DocTr: Document Image Transformer for Geometric Unwarping and Illumination Correction<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Hao Feng*; Yuechen Wang; Wengang Zhou; Jiajun Deng; Houqiang Li</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1348 <b>Multimodal Entity Linking: A New Dataset and A Baseline<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jingru Gan*; Jinchang Luo; Haiwei Wang; Shuhui Wang; Wei He; Qingming Huang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1388 <b>ReconVAT: A Semi-Supervised Automatic Music Transcription Framework for Low-Resource Real-World Data<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Kin Wai Cheuk*; Dorien Herremans; Li Su</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1392 <b>Learning Unified Embeddings for Recommendation via Meta-path Semantics<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Qianxiu Hao*; Qianqian Xu; Zhiyong Yang; Qingming Huang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1394 <b>Video Semantic Segmentation with Sparse Temporal Transformer<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jiangtong Li*; Wentao Wang; Junjie Chen; Li Niu; Jianlou Si; Chen Qian; Liqing Zhang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1410 <b>iButter: Neural Interactive Bullet Time Generator for Human Free-viewpoint Rendering<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Liao Wang*; Ziyu Wang; Pei Lin; Yuheng Jiang; Xin Suo; Minye Wu; Lan Xu; Jingyi Yu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1429 <b>Elastic Tactile Simulation Towards Tactile-Visual Perception<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yikai Wang*; Wenbing Huang; Bin Fang; Fuchun Sun; Chang Li</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1447 <b>Self-supervised Consensus Representation Learning for Attributed Graph<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Changshu Liu*; Liangjian Wen; Zhao Kang; Guangchun Luo; Ling Tian</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1465 <b>Learning Fine-Grained Motion Embedding for Landscape Animation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Hongwei Xue; Bei Liu*; Huan Yang; Jianlong Fu; Houqiang Li; Jiebo Luo</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1474 <b>Video-to-Image Casting: A Flatting Method for Video Analysis<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xu Chen*; Chenqiang Gao; Feng Yang; Xiaohan Wang; Yi Yang; Yahong Han</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1486 <b>Automated Playtesting with a Cognitive Model of Sensorimotor Coordination<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Injung Lee; Hyunchul Kim; Byungjoo Lee*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1500 <b>Diverse Image Inpainting with Bidirectional and Autoregressive Transformers<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yingchen Yu; Fangneng Zhan; Rongliang WU; Jianxiong Pan; Kaiwen Cui; Shijian Lu*; Feiying Ma; Xuansong Xie; Chunyan Miao</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1508 <b>FMSing: Fast Multi-Singer Vocoder with A large-Scale Singing Voice Corpus<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Rongjie Huang*; Feiyang Chen; Yi Ren; Jinglin Liu; Chenye Cui; Zhou Zhao</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1510 <b>TSA-Net: Tube Self-Attention Network for Action Quality Assessment<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Shunli Wang; Dingkang Yang; Peng Zhai; Chixiao Chen; Lihua Zhang*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1537 <b>Neural Free-Viewpoint Performance Rendering under Complex Human-object Interactions<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Guoxing Sun*; Xin Chen; Yizhang Chen; Anqi Pang; Pei Lin; Yuheng Jiang; Lan Xu; Jingyi Yu; Jingya Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1542 <b>Multifocal Attention-Based Cross-Scale Network for Image De-raining<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zheyu Zhang; Yurui Zhu; Xueyang Fu*; Zhiwei Xiong; Zheng-Jun Zha; Feng Wu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1561 <b>Learning Disentangled Factors from Paired Data in Cross-Modal Retrieval: An Implicit Identifiable VAE Approach<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Minyoung Kim*; Ricardo Guerrero ; Vladimir Pavlovic</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1573 <b>A Novel Patch Convolutional Neural Network for View-based 3D Model Retrieval<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zan Gao; Yuxiang Shao; Weili Guan; Meng Liu; Zhiyong Cheng*; Shengyong Chen</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1584 <b>Meta Self-Paced Learning for Cross-Modal Matching<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jiwei Wei; Xing Xu*; Zheng Wang; Guoqing Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1606 <b>SSFlow: Style-guided neural Spline Flows for Face Image Manipulation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Hanbang Liang; Xianxu Hou; Linlin Shen*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1614 <b>Contrastive Disentangled Meta-Learning for Signer-Independent Sign Language Translation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Tao Jin*; Zhou Zhao</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1627 <b>Personality Recognition by Modelling Person-specific Cognitive Processes using Graph Representation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zilong Shao; Siyang Song*; Shashank Jaiswal; Linlin Shen; Michel Valstar; Hatice Gunes</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1676 <b>DPT: Deformable Patch-based Transformer for Visual Recognition<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhiyang Chen*; Yousong Zhu; Chaoyang Zhao; Guosheng Hu; Wei Zeng; Jinqiao Wang; Ming Tang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1678 <b>MeshNet++: A Network with a Face<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Vinit Veerendraveer Singh*; Shivanand Venkanna Sheshappanavar; Chandra Kambhamettu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1681 <b>Scene Text Image Super-Resolution via Parallelly Contextual Attention Network<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Cairong Zhao *; Shuyang Feng; Brain Nlong Zhao; Zhijun Ding; Jun Wu; Fumin Shen; Heng Tao Shen</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1686 <b>Is Visual Context Really Helpful for Knowledge Graph? A Representation Learning Perspective<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Meng Wang*; Sen Wang; Han Yang; Zheng Zhang; Xi Chen; Guilin Qi</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1703 <b>EVRNet: Efficient Video Restoration on Edge Devices<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Sachin Mehta*; Amit Kumar; Fitsum Reda; Varun Nasery; Vikram Mulukutla; Rakesh Ranjan; Vikas Chandra</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1732 <b>InterBN: Channel Fusion for Adversarial Unsupervised Domain Adaptation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Mengzhu Wang; Wei Wang; Baopu Li*; Xiang Zhang; Long Lan; Tan H Huibin; Tianyi Liang; Wei Yu; Zhigang Luo</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1748 <b>Perceptual Quality Assessment of Internet Videos<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jiahua Xu; Jing Li*; XingGuang Zhou; Wei Zhou; Baichao Wang; Zhibo Chen</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1763 <b>Exploring Pathologist Knowledge for Automatic Assessment of Breast Cancer Metastases in Whole-slide Image<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Liuan Wang*; Li Sun; Mingjie Zhang; Huigang Zhang; Wang Ping; Rong Zhou; Jun Sun</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1783 <b>Complementary Trilateral Decoder for Fast and Accurate Salient Object Detection<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhirui Zhao; Changqun Xia*; Chenxi Xie; Jia Li</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1798 <b>Constrained Graphic Layout Generation via Latent Optimization<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Kotaro Kikuchi*; Edgar Simo-Serra; Mayu Otani; Kota Yamaguchi</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1812 <b>Transfer Vision Patterns for Multi-Task Pixel Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xiaoya Zhang; Ling Zhou; Yong Li; Zhen Cui*; Jin Xie; Jian Yang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1815 <b>AI-Lyricist: Generating Music and Vocabulary Constrained Lyrics<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xichu Ma; Ye Wang*; Min-Yen Kan; Wee Sun Lee</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1828 <b>Theophany: Multimodal Speech Augmentation in Instantaneous Privacy Channels<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Abhishek Kumar*; Tristan Braud; Lik Hang Lee; Pan Hui</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1843 <b>Video Representation Learning with Graph Contrastive Augmentation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jingran Zhang*; Xing Xu; Fumin Shen; Yazhou Yao; Jie Shao; Xiaofeng Zhu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1859 <b>Why Do We Click: Visual Impression-aware News Recommendation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jiahao Xun*; Shengyu Zhang; Zhou Zhao; Jieming Zhu; Qi Zhang; Jingjie Li; Xiuqiang He; Xiaofei He; Tat-Seng Chua; Fei Wu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1892 <b>MeronymNet: A Hierarchical Model for Unified and Controllable Multi-Category Object Generation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Rishabh Baghel; Abhishek Trivedi; Tejas Ravichandran; Ravi Kiran Sarvadevabhatla*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1901 <b>ArtScience and the ICECUBE LED Display [ILDm^3]<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Mark-David Hosale*; Robert Allison; Jim Madsen; Marcus Gordon</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1907 <b>Diverse Multimedia Layout Generation with Multi Choice Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">David D Nguyen*; Surya Nepal; Salil Kanhere</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1908 <b>Semi-supervised Domain Adaptive Retrieval via Discriminative Hashing Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Haifeng Xia; Taotao Jing; Chen Chen; Zhengming Ding*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1913 <b>Semi-supervised Learning via Improved Teacher-Student Network For Robust 3D Reconstruction of Stereo Endoscopic Image<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Hongkuan Shi; Zhiwei Wang; Jinxin Lv; Yilang Wang; Peng Zhang; Fei Zhu; Qiang Li*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1922 <b>Actions speak louder than listening: evaluating music style transfer based on editing experience<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Wei-Tsung Lu; Meng-Hsuan Wu; Yuh-Ming Chiu; Li Su*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1932 <b>Towards Accurate Localization by Instance Search<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yi-Geng Hong; Hui-Chu Xiao; Wan-Lei Zhao*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1939 <b>Object-aware Long-short-range Spatial Alignment for Few-Shot Fine-Grained Image Classification<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yike Wu; Bo Zhang; Gang Yu; Weixi Zhang; Bin Wang; Tao Chen*; Jiayuan Fan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1970 <b>CoReD: Generalizing Fake Media Detection with Continual Representation using Distillation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Minha Kim; Shahroz Tariq*; Simon Woo</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1985 <b>I Know Your Keyboard Input: A Robust Keystroke Eavesdropper Based-on Acoustic Signals<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jia-Xuan Bai*; Bin Liu; Luchuan Song</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2002 <b>Towards Multiple Black-boxes Attack via Adversarial Example Generation Network<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Duan Mingxing; Kenli Li; Lingxi Xie; Qi Tian*; Bin Xiao</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2056 <b>Dense Semantic Contrast for Self-Supervised Visual Representation Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xiaoni Li; Yu Zhou*; Yifei Zhang; aoting zhang; Wei Wang; Ning Jiang; Haiying Wu; Weiping Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2067 <b>Enhancing Knowledge tracing via adversarial training<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xiaopeng Guo*; Zhijie Huang; Jie Gao; Mingyu Shang; Maojing shu; Jun Sun</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2094 <b>Multimodal Asymmetric Dual Learning for Unsupervised Eyeglasses Removal<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Qing Lin; Bo Yan*; Weimin Tan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2096 <b>Deep Marginal Fisher Analysis based CNN for Image Representation and Classification<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xun Cai; Jiajing Chai; Yanbo Gao*; Shuai Li; Bo Zhu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2107 <b>Learning Structure Affinity for Video Depth Estimation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yuanzhouhan Cao; Yidong Li*; Haokui Zhang; Chao Ren; Yifan Liu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2113 <b>Viewing From Frequency Domain: A DCT-based Information Enhancement Network for Video Person Re-Identification<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Liangchen Liu; Xi Yang*; Nannan Wang; Xinbo Gao</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2166 <b>Improving Pedestrian Detection from a Long-tailed Domain Perspective<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Mengyuan Ding*; Shanshan Zhang; Jian Yang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2184 <b>MusicBERT: A Self-supervised Learning of Music Representation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Hongyuan Zhu*; Ye Niu; Di Fu; Hao Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2194 <b>Structure-aware Mathematical Expression Recognition with Sequence-Level Modeling<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Minli Li; Peilin Zhao; Yifan Zhang; Shuaicheng Niu; Qingyao Wu; Mingkui Tan*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2203 <b>Informative Class-Conditioned Feature Alignment for Unsupervised Domain Adaptation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Wanxia Deng; Yawen Cui; Zhen Liu; Gangyao Kuang; Dewen Hu; Matti Pietikinen; Li Liu*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2235 <b>HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent Neural Network for Multi-modal Emotion Recognition<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Ziyu Jia; Youfang Lin; Jing Wang*; Zhiyang Feng; Xiangheng Xie; Caijie Chen</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2261 <b>Graph Neural Networks for Knowledge Enhanced Visual Representation of Paintings<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Athanasios Efthymiou*; Stevan Rudinac; Monika Kackovic; Marcel Worring; Nachoem Wijnberg</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2275 <b>Is Someone Speaking? Exploring Long-term Temporal Features for Audio-visual Active Speaker Detection<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Ruijie Tao*; Zexu Pan; Rohan Kumar Das; Xinyuan Qian; Mike Zheng Shou; Haizhou Li</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2288 <b>ChartPointFlow for Topology-Aware 3D Point Cloud Generation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Takumi Kimura; Takashi Matsubara*; Kuniaki Uehara</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2297 <b>TBRA: Tiling and Bitrate Adaptation for Mobile 360-Degree Video Streaming<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Lei Zhang*; Yanyan Suo; Ximing Wu; Feng Wang; Yuchi Chen; Laizhong Cui; Jiangchuan Liu; Zhong Ming</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2335 <b>DeepGame: Efficient Video Encoding for Cloud Gaming<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Omar Mossad*; Khaled Diab; Ihab Amer; Mohamed Hefeeda</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2340 <b>Cross-View Exocentric to Egocentric Video Synthesis<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Gaowen Liu; Hao Tang*; Hugo M Latapie; Jason J Corso; Yan Yan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2365 <b>Graph Convolutional Multi-modal Hashing for Flexible Multimedia Retrieval<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xu Lu; Lei Zhu; Li Liu; Liqiang Nie; Huaxiang Zhang*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2405 <b>Two-pronged Strategy: Lightweight Augmented Graph Network Hashing for Scalable Image Retrieval<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Hui Cui; Lei Zhu*; Jingjing Li; Zhiyong Cheng; Zheng Zhang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2406 <b>Beyond OCR + VQA: Involving OCR into the Flow for Robust and Accurate TextVQA<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Gangyan Zeng; Yuan Zhang; Yu Zhou*; Xiaomeng Yang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2427 <b>Direction Relation Transformer for Image Captioning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zeliang Song; Xiaofei Zhou*; Linhua Dong; Jianlong Tan; Li Guo</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2447 <b>Generating Point Cloud from Single Image in The Few Shot Scenario<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yu Lin*; Jinghui Guo; Yang Gao; Yi-Fan Li; Zhuoyi Wang; Latifur Khan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2469 <b>CAA: Candidate-Aware Aggregation for Temporal Action Detection<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yifan Ren; Xing Xu*; Fumin Shen; Yazhou Yao; Huimin Lu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2483 <b>Progressive Semantic Matching for Video-Text Retrieval<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Hongying Liu; Ruyi Luo; Fanhua Shang*; Mantang Niu; Yuanyuan Liu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2486 <b>Co-learning: Learning from noisy labels with self-supervision<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Cheng Tan*; Jun Xia; Lirong Wu; Stan Z. Li</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2488 <b>AITransfer: Progressive AI-powered Transmission for Real-Time Point Cloud Video Streaming<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yakun Huang*; Yuanwei Zhu; Xiuquan Qiao; Zhijie Tan; Boyuan Bai</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2495 <b>SRNet: Spatial Relation Network for Efficient Single-stage Instance Segmentation in Videos<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xiaowen Ying*; Xin Li; Mooi Choo Chuah</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2503 <b>Learning Human Motion Prediction via Stochastic Differential Equations<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Kedi Lyu*; Zhenguang Liu; Shuang Wu; Haipeng Chen; Xuhong Zhang; Yuyu Yin</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2513 <b>Using Interaction Data to Predict Engagement with Interactive Media<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jonathan Carlton*; Andy Brown; Dr.Caroline Jay; John Keane</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2519 <b>Disentangled Representation Learning and Enhancement Network for Single Image De-Raining<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Guoqing Wang; Changming Sun; Xing Xu; Jingjing Li; Zheng Wang*; Zeyu Ma</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2546 <b>Space-Time Interaction Graph Parsing Networks for Human-Object Interaction Recognition<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Ning Wang; Guangming Zhu*; Liang Zhang; Peiyi Shen; Hongsheng Li; Cong Hua</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2588 <b>A Stepwise Matching Method for Multimodal Image based on Cascaded Network<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jinming Mu; Shuiping Gou; Shasha Mao*; Shankui Zheng</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2596 <b>Zero-shot Video Emotion Recognition via Multimodal Protagonist-aware Transformer Network<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Fan Qi*; Xiaoshan Yang; Changsheng Xu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2608 <b>Knowledge perceived multi-modal pretraining in E-commerce<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yushan Zhu*; Huaixiao Zhao; Wen Zhang; Ganqiang Ye; Hui Chen; Ningyu Zhang; Huajun Chen</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2629 <b>PFFN: Progressive Feature Fusion Network for Lightweight Image Super-Resolution<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Dongyang Zhang; Changyu Li; Ning Xie; Guoqing Wang; Jie Shao*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2659 <b>Game Theory-driven Rate Control for 360-Degree Video Coding<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Tiesong Zhao*; Jielian Lin; Yanjie Song; Xu Wang; Yuzhen Niu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2687 <b>Differentiated Learning for Multi-Modal Domain Adaptation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jianming Lv*; Kaijie Liu; Shengfeng He</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2691 <b>Learning What and When to Drop: Adaptive Multimodal and Contextual Dynamics for Emotion Recognition in Conversation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Feiyu Chen; Zhengxiao Sun; Deqiang Ouyang; Xueliang Liu; Jie Shao*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2703 <b>Unsupervised Portrait Shadow Removal via Generative Prior<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yingqing HE; Yazhou Xing; Tianjia Zhang; Qifeng Chen*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2773 <b>Self-Supervised Regional and Temporal Auxiliary Tasks for Facial Action Unit Recognition<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jingwei Yan*; Jingjing Wang; Qiang Li; Chunmao Wang; Shiliang Pu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2799 <b>Exploring Logical Reasoning for Referring Expression Comprehension<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Ying Cheng*; Ruize Wang; Jiashuo Yu; Rui-Wei Zhao; Yuejie Zhang; Rui Feng</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2805 <b>aBio: Active Bi-Olfactory Display Using Subwoofers for Virtual Reality<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">You-Yang Hu*; Yao-Fu Jan; Kuan-Wei Tseng; You-Shin Tsai; Hung-Ming Sung; Jin-Yao Lin; Yi-Ping Hung </p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2829 <b>Database-adaptive Re-ranking for Enhancing Cross-modal Image Retrieval<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Rintaro Yanagi*; Ren Togo; Takahiro Ogawa; Miki Haseyama</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2840 <b>Fast video visual quality and resolution improvement using SR-UNet<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Federico Vaccaro; Marco Bertini*; Tiberio Uricchio; Alberto Del Bimbo</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2850 <b>Linking the Characters: Video-oriented Social Graph Generation via Hierarchical-cumulative GCN<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Shiwei Wu*; Joya Chen; Tong Xu; Liyi Chen; Lingfei Wu; Yao Hu; Enhong Chen</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2883 <b>Human attributes prediction under privacy-preserving conditions<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Anshu Singh*; Shaojing Fan; Mohan Kankanhalli</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2896 <b>Deep Unsupervised 3D SfM Face Reconstruction Based on Massive Landmark Bundle Adjustment<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yuxing Wang; Yawen Lu; Zhihua Xie; Guoyu Lu*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2902 <b>Collocation and Try-on Network: Whether an outfit is Compatible<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Na Zheng*; Xuemeng Song; Qingying Niu; Xue Dong; Yibing Zhan; Liqiang Nie</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2905 <b>Exploiting BERT For Multimodal Target Sentiment Classification Through Input Space Translation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zaid Khan*; Yun Fu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2918 <b>Air-Text: Air-Writing and Recognition System<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Sun-Kyung Lee; Jong-Hwan Kim*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2920 <b>Multi-label Pattern Image Retrieval via Attention Mechanism Driven Graph Convolutional Network<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Ying Li*; Hongwei Zhou; Yeyu Yin; Jiaquan Gao</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2954 <b>Partial Tubal Nuclear Norm Regularized Multi-view Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yongyong Chen*; Shuqin Wang; Chong Peng; Guangming Lu; Yicong Zhou</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2970 <b>Simplifying Multimodal Emotion Recognition with Single Eye Movement Modality<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xu Yan*; Liming Zhao; Bao-Liang Lu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2982 <b>Cross-View Representation Learning for Multi-View Logo Classification with Information Bottleneck<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jing Wang; Yuanjie Zheng*; Jingqi Song; Sujuan Hou</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">3023 <b>Pre-training Graph Transformer with Multimodal Side Information for Recommendation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yong Liu*; Susen Yang; Chenyi Lei; Guoxin Wang; Haihong Tang; Juyong Zhang; Aixin Sun; Chunyan Miao</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">3052 <b>Learning to Compose Stylistic Calligraphy Artwork with Emotions<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Shaozu Yuan; Ruixue Liu; Meng Chen*; Baoyang Chen; Zhijie Qiu; Xiaodong He</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">3063 <b>Scene Graph with 3D Information for Change Captioning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zeming Liao; Qingbao Huang*; Yu Liang; Mingyi Fu; Yi Cai; Qing Li</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="primaryTitle el-row"><div data-v-fe7dd554="" class="el-col el-col-2"><img data-v-fe7dd554="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEQAAABECAYAAAA4E5OyAAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAARKADAAQAAAABAAAARAAAAADA2cO/AAAE+0lEQVR4Ae2aW6gVVRjH7WqZFgndKI+HA1EZaCUEXVF7KIwolBAhg3oMScIHMYoIfBMKeim6URBdKajwwYoQCiLzSdTEsocULLuZhlli9fvD/mDOOmvWrDUzezu7vT74s2bW+tZ3+e+ZdZs9bVqWzEBmIDPQfQauJsR3wEHwE/gI3AVGUlaR9XHwrwfPUncKGBm5lkxPAB8ZVrd6ZNgg0TcqyBAp348SId9EECJSxrtCyul9DmQ79o9G+BipcSSCj6ySGcgMZAYyA5mBzEBmIDOQGcgMhBkY1B7iVMLQIdEEmANmgNPAIbAJjNSO90YSPgDs/MMtD9M2UqdnbwfIMHJ+ROcs0Hm5jgjXgEsaRLqYvnoKLPmycryBj753nYWHD4AFf0VDjxo3Xi/YM7tWirBOPyGfOsGPcd+G3ISRr4ARYeUzLRjXBDGzBTtTTKygxgK1ct4UrfoVCvw+IGJ+Aa+COk+HZq7bwEvga3AMKN6/gWatV8C94AzQSIqvihFyc4XFB2jX+amSG4QswclOYPGFyr3orQL6IWrJz/RyHTwcsLSUtn96ff6i7Mtj2/OvtYu+5bjxxdy/T79ze3aSiiMeh++VWNDj+IOjv6hEt2n12RjwPb0xZJjOLmxcnBrIVjqYASv11Pgeubs9umsDDqfT9iC4NaBT1vQaDRZPk/JL7CSNWetLHF/jiVQDmhucplefjFO5DUhfBKfIQyi7fprcv5DifAbK+z0B+BLd4tHbQ50rd1LxK7AktrsKgft5tNkMYv2blhrz9Jk1WjRruE5lZL5jQdOdq3eCOg1+Jsu5sEFXukpO+5sY0Yc037rF9VnnfnNMAKajpbrPyYem0Cs/LtG7oKBXXKGKwOsLbVWXj6Hgi6ONOv1Il1YFYO0aicucarFj8jQXrt4+6rRoMhnn4kmwDBSfHG6DsoBWLbJc+23ePxKMoNAYIkTfau1XnuBaZxoW5B9cFwnjtpacT6/YhZf5rlO+GxvdOSiGHOh8Y6xnTMHfD+4Bc3p1TQr5/gKE/LfVtiMl0IMVQWmRc3mKwQhdrQ8+AW0lXGVHT3S0PI9mlcHD6KyIthhW1H7JN2tVxdCk/fdwSJNbb+c21tkmdBdP7h59pxntOVCcmmP9NtXTWxAt2qf8BlKcaiW6DtwAzgRlokFbS3i9Hlq3pPhoU1drnEkS+gfRcTS1mdKAGSsLURQkf4JvwZEeNIXOBZqZZoIuyNbUIHQa3uYv0jVbK1MJ0UJKs0nXEmkjHp3WTdn1FleUPrL0fj/ua/gf1L1MDsfq5KFzENu2t/HLdMGGVtcX1SHD+qRMwV1IuCqGNZZYk3ILnascDUO7zmJCs2s0R+5GbhiSd2PUmHhLdMYRitrAuU6G6f6JiByTVTYOKSk6zKqaVZPJUAe9f5+BYXoy9hPvhaBvor2IzhKGgRT91eKqvjFRMDyb60Ed5NQlXqtR92C8kEL7lzrd2gzqBtzPfvqiaBvN9jMPWNQ2/03Qz+RSbevr3GWBmAfSpG85xY9RqUm0pf8icUwfSMYRTrQ/eAu0lVyKnT34vSMixpOionOUvSAlobq6Ohd9FIRO6Gg++aJF0FKgUzctmesmXNZvNzZXg1lg6GSMiDeA70BZgjH1+nj1FFgCdCzRuvTFaEWUc2lfBLQ+uBJMAH3sOg9oFXy0B60uNS4YPudan0mzZAYyA5mBzjLwH3H7ah8z4OG+AAAAAElFTkSuQmCC" alt="" class="icon"></div><div data-v-fe7dd554="" class="el-col el-col-22"><p data-v-fe7dd554="" class="title">Poster Papers</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">23 <b>JPGNet: Joint Predictive Filtering and Generative Network for Image Inpainting<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Qing Guo*; Xiaoguang Li; Felix Juefei-Xu; Hongkai Yu; Yang Liu; Song Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">29 <b>AdvFilter: Predictive Perturbation-aware Filtering against Adversarial Attack via Multi-domain Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yihao Huang; Qing Guo*; Felix Juefei-Xu; Lei Ma; Weikai Miao; Yang Liu; Geguang Pu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">86 <b>Pixel-level Intra-domain Adaptation for Semantic Segmentation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zizheng Yan*; Xianggang Yu; Yipeng Qin; Yushuang Wu; Xiaoguang Han; Shuguang Cui</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">103 <b>Mask is All You Need: Rethinking Mask R-CNN for Dense and Arbitrary-Shaped Scene Text Detection<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xugong Qin; Yu Zhou*; Youhui Guo; Dayan Wu; Zhihong Tian; Ning Jiang; Hongbin Wang; Weiping Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">132 <b>Windowing Decomposition Convolutional Neural Network for Image Enhancement<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Chuanjun Zheng*; Daming Shi; Yukun Liu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">146 <b>Joint Optimization in Edge-Cloud Continuum for Federated Unsupervised Person Re-identification<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Weiming Zhuang*; Yonggang Wen; Shuai Zhang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">156 <b>Multi-view 3D Smooth Human Pose Estimation based on Heatmap Filtering and Spatio-temporal Information<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zehai Niu; Ke Lu; Jian Xue*; Haifeng Ma; Runchen Wei</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">162 <b>Imitative Learning for Multi-Person Action Forecasting<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yuke Li*; Pin Wang; Mang Ye; Ching-Yao Chan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">182 <b>Stereo Video Super-Resolution via Exploiting View-Temporal Correlations<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Ruikang Xu; Zeyu Xiao; Mingde Yao; Yueyi Zhang*; Zhiwei Xiong</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">195 <b>M3TR: Multi-modal Multi-label Recognition with Transformer<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jiawei Zhao; Yifan Zhao; Jia Li*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">211 <b>TACR-Net: Editing on Deep Video and Voice Portraits<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Luchuan Song*; Bin Liu; Guojun Yin; Xiaoyi Dong; Yufei Zhang; Jia-Xuan Bai</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">230 <b>Annotation-Efficient Untrimmed Video Action Recognition<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yixiong Zou*; Shanghang Zhang; Guangyao Chen; Yonghong Tian; Kurt Keutzer; Jos M. F. Moura</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">234 <b>Face-based Voice Conversion: Learning the Voice behind a Face<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Hsiao-Han Lu*; Shao-En Weng; Ya-Fan Yen; Hong-Han Shuai; Wen-Huang Cheng</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">242 <b>A Large-Scale Benchmark for Food Image Segmentation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xiongwei Wu*; Xin Fu; Ying Liu; Ee-peng Lim; Steven Hoi; Qianru Sun</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">243 <b>HAT: Hierarchical Aggregation Transformers for Person Re-identification<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Guowen Zhang; Pingping Zhang; Jinqing Qi; Huchuan Lu*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">245 <b>Long-Range Feature Propagating for Natural Image Matting<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Qinglin Liu*; Haozhe Xie; Shengping Zhang; Bineng Zhong; Rongrong Ji</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">263 <b>Towards Controllable and Photorealistic Region-wise Image Manipulation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Ansheng You*; Chenglin Zhou; Qixuan Zhang; Lan Xu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">266 <b>Information-Growth Attention Network for Image Super-Resolution<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhuangzi Li*; Ge Li; Thomas H Li; Shan Liu; Wei Gao</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">269 <b>Anchor-free 3D Single Stage Detector with Mask-Guided Attention for Point Cloud<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jiale Li; Hang Dai*; Ling Shao; Yong Ding</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">275 <b>Shape Controllable Virtual Try-on for Underwear Models<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xin Gao*; Zhenjiang Liu; Zunlei Feng; Chengji Shen; Kairi Ou; Haihong Tang; Mingli Song</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">278 <b>E^2Net: Excitative-Expansile Learning for Weakly Supervised Object Localization<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhiwei Chen*; Liujuan Cao; Yunhang Shen; Feihong Lian; Yongjian Wu; Rongrong Ji</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">314 <b>Few-shot Fine-Grained Action Recognition via Bidirectional Attention and Contrastive Meta-Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jiahao Wang*; Yunhong Wang; Sheng Liu; Annan Li</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">325 <b>Selective Dependency Aggregation for Action Classification<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yi Tan; Yanbin Hao*; Xiangnan He; Yinwei Wei; Xun Yang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">335 <b>Conditional Directed Graph Convolution for 3D Human Pose Estimation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Wenbo Hu*; Changgong Zhang; Fangneng Zhan; Lei Zhang; Tien-Tsin Wong</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">339 <b>Cross Chest Graph for Disease Diagnosis with Structural Relational Reasoning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Gangming Zhao*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">355 <b>ZiGAN:Fine-grained Chinese Calligraphy Font Generation via a Few-shot Style Transfer Approach<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Qi Wen; Shuang Li; Bingfeng Han; Yi Yuan*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">359 <b>Cycle-Consistent Inverse GAN for Text-to-Image Synthesis<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Hao Wang*; Guosheng Lin; Steven Hoi; Chunyan Miao</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">366 <b>Fully Quantized Image Super-Resolution Networks<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Hu Wang*; Peng Chen; Bohan Zhuang; Chunhua Shen</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">369 <b>AKECP: Adaptive Knowledge Extraction from Feature Maps for Fast and Efficient Channel Pruning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Haonan Zhang; Longjun Liu*; Hengyi Zhou; Wenxuan Hou; Hongbin Sun; Nanning Zheng</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">383 <b>Dynamic Momentum Adaptation for Zero-Shot Cross-Domain Crowd Counting<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Qiangqiang Wu*; Jia Wan; Antoni Chan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">393 <b>Auto-MSFNet: Search Multi-scale Fusion Network for Salient Object Detection<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Miao Zhang; Tingwei Liu; Yongri Piao*; Shunyu Yao; Huchuan Lu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">402 <b>Few-shot Unsupervised Domain Adaptation with Image-to-Class Sparse Similarity Encoding<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Shengqi Huang; Wanqi Yang*; Lei Wang; Luping Zhou; Ming Yang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">403 <b>Semantic-aware Transfer with Instance-adaptive Parsing for Crowded Scenes Pose Estimation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xuanhan Wang*; Lianli Gao; Yan Dai; Yixuan Zhou; Jingkuan Song</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">411 <b>Multimodal Dialog System: Relational Graph-based Context-aware Question Understanding<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Haoyu Zhang; Meng Liu; Zan Gao; Xiaoqiang Lei; Yinglong Wang; Liqiang Nie*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">413 <b>Shadow Detection via Predicting the Confidence Maps of Shadow Detection Methods<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">JingWei Liao*; Yanli Liu; GuanYu Xing; Housheng Wei; JueYu Chen; Songhua Xu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">424 <b>TEID: A New State-of-the-art Approach for Future Motion Prediction<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Pengxiang Su*; Zhenguang Liu; Shuang Wu; Lei Zhu; Yifang Yin; Xuanjing Shen</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">431 <b>Q-Art Code: Generating Scanning-robust Art-style QR Codes by Deformable Convolution<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Hao Su*; jianwei Niu; Xuefeng Liu; Qingfeng Li; Ji Wan; Mingliang Xu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">433 <b>Depth Quality-Inspired Feature Manipulation for Efficient RGB-D Salient Object Detection<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Wenbo Zhang; Ge-Peng Ji; Zhuo Wang; Keren Fu*; Qijun Zhao</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">441 <b>Revisiting Mid-Level Patterns for Cross-Domain Few-Shot Recognition<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yixiong Zou*; Shanghang Zhang; Jianpeng Yu; Yonghong Tian; Jos M. F. Moura</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">451 <b>Space-Angle Super-Resolution for Multi-View Images<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yuqi Sun; Ri Cheng; Bo Yan*; Shili Zhou</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">461 <b>Weakly-Supervised Video Object Grounding via Stable Context Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Wei Wang*; Junyu Gao; Changsheng Xu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">477 <b>Modeling the Uncertainty for Self-supervised 3D Skeleton Action Representation Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yukun Su; Guosheng Lin; RuiZhou Sun; Yun Hao; Qingyao Wu*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">482 <b>D3Net: Dual-Branch Disturbance Disentangling Network for Facial Expression Recognition<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Rongyun Mo; Yan Yan*; Jing-Hao Xue; Si Chen; Hanzi Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">488 <b>Towards a Unified Middle Modality Learning for Visible-Infrared Person Re-Identification<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhang Yukang; Yan Yan; Yang Lu; Hanzi Wang*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">493 <b>ROSITA: Enhancing Vision-and-Language Semantic Alignments via Cross- and Intra-modal Knowledge Integration<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yuhao Cui; Zhou Yu*; Chunqi Wang; Zhongzhou Zhao; Ji Zhang; Meng Wang; Jun Yu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">497 <b>Object Point Cloud Classification via Poly-ConvolutionalArchitecture Search<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xuanxiang Lin*; Ke Chen; Kui Jia</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">499 <b>Semantic-Guided Relation Propagation Network for Few-shot Action Recognition<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xiao Wang; Weirong Ye; Zhongang Qi; Xun Zhao; Guangge Wang; Ying Shan; Hanzi Wang*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">505 <b>Anti-Distillation Backdoor Attacks: Backdoors Really Can Survive in The Knowledge Distillation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yunjie Ge*; Qian Wang; Baolin Zheng; Xinlu Zhuang; Qi Li; Chao Shen; Cong Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">519 <b>One-stage Context and Identity Hallucination Network<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yinglu Liu*; Mingcan Xiang; Hailin Shi; Tao Mei</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">520 <b>Enhancing Generalized Zero-Shot Learning by Visual Perturbation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhi Chen*; Yadan Luo; Sen Wang; Ruihong Qiu; Jingjing Li; Zi Huang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">528 <b>Weakly-Supervised Temporal Action Localization via Cross-Stream Collaborative Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yuan Ji*; Xu Jia; Huchuan Lu; Xiang Ruan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">537 <b>Deep Interactive Video Inpainting: an Invisibility Cloak for Harry Potter<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Cheng Chen*; Jiayin Cai; Yao Hu; Xu Tang; Xinggang Wang; Chun Yuan; Xiang Bai; Song Bai</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">552 <b>Searching Motion Graphs for Human Motion Synthesis<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Chenchen Liu; Yadong Mu*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">555 <b>When Video Classification Meets Incremental Classes<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Hanbin Zhao; Xin Qin; Shihao Su; Yongjian Fu; Zibo Lin; Xi Li*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">559 <b>Fast and Accurate Lane Detection via Frequency Domain Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yulin He*; Wei Chen; Zhengfa Liang; Dan Chen; Yusong Tan; Xin Luo; Chen Li; Yulan Guo</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">560 <b>Learning Multi-context Aware Location Representations from Large-scale Geotagged Images<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yifang Yin*; Ying Zhang; Zhenguang Liu; Yuxuan Liang; Sheng Wang; Rajiv Ratn Shah; Roger Zimmermann</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">562 <b>MV-TON: Memory-based Video Virtual Try-on network<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xiaojing Zhong; Zhonghua Wu; Taizhe Tan; Guosheng Lin; Qingyao Wu*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">568 <b>Token Shift Transformer for Video Classification<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Hao Zhang; Yanbin Hao*; Chong-Wah Ngo</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">576 <b>Attribute-specific Control Units in StyleGAN for Fine-grained Image Manipulation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Rui Wang; Jian Chen; Gang Yu; Li Sun; Changqian Yu; Changxin Gao*; Nong Sang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">588 <b>Attention-driven Graph Clustering Network<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhihao Peng; Hui Liu; Yuheng Jia; Junhui Hou*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">590 <b>Lifting the Veil of Frequency in Joint Segmentation and Depth Estimation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Tianhao Fu*; Yingying Li; Xiaoqing Ye; Xiao Tan; Hao Sun; Fumin Shen; Errui Ding</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">594 <b>Visual Co-Occurrence Alignment Learning for Weakly-Supervised Video Moment Retrieval<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zheng Wang*; Jingjing Chen; Yu-Gang Jiang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">606 <b>Adaptive Normalized Representation Learning for Generalizable FaceAnti-Spoofing<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">ShuBao Liu*; Ke-Yue Zhang; Taiping Yao; Mingwei Bi; Shouhong Ding; Jilin Li; Feiyue Huang; Lizhuang Ma</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">609 <b>Imitating Arbitrary Talking Style for Realistic Audio-Driven Talking Face Synthesis<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Haozhe Wu*; Jia Jia; Haoyu Wang; Yishun Dou; Chao Duan; Qingshan Deng</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">618 <b>Pose-guided Inter-and Intra-part Relational Transformer for Occluded Person Re-Identification<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhongxing Ma; Yifan Zhao; Jia Li*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">624 <b>VLAD-VSA: Cross-Domain Face Presentation Attack Detection with Vocabulary Separation and Adaptation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jiong Wang; Zhou Zhao*; Weike Jin; Xinyu Duan; Zhen Lei; Baoxing Huai; Yiling Wu; Xiaofei He</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">651 <b>End-to-End Video Object Detection with Spatial-Temporal Transformers<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Lu He*; Qianyu Zhou; Xiangtai Li; Li Niu; Guangliang Cheng; Xiao Li; Wenxuan Liu; Yunhai Tong; Lizhuang Ma; Liqing Zhang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">653 <b>Joint-teaching: Learning to Refine Knowledge for Resource-constrained Unsupervised Cross-modal Retrieval<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Peng-Fei Zhang*; Jiasheng Duan; Zi Huang; Hongzhi Yin</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">665 <b>AggNet for Self-supervised Monocular Depth Estimation: Go An Aggressive Step Furthe<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhi Chen; Xiaoqing Ye; Liang Du; Wei Yang*; Liusheng Huang; Xiao Tan; Zhenbo Shi; Fumin Shen; Errui Ding</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">681 <b>Boosting Lightweight Single Image Super-resolution via Joint-distillation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xiaotong Luo*; Qiuyuan Liang; Ding Liu; Yanyun Qu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">688 <b>Discriminator-free Generative Adversarial Attack<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Shaohao Lu; Yuqiao Xian; Ke Yan; Yi Hu; Xing Sun; Xiaowei Guo; Feiyue Huang; Wei-Shi Zheng*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">691 <b>Former-DFER: Dynamic Facial Expression Recognition Transformer<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zengqun Zhao; Qingshan Liu*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">702 <b>Discovering Density-Preserving Latent Space Walks in GANs for Semantic Image Transformations<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Guanyue Li; Yi Liu; Xiwen Wei; Yang Zhang; Si Wu*; Yong Xu; Hau San Wong</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">715 <b>MGH: Metadata Guided Hypergraph Modeling for Unsupervised Person Re-identification<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yiming Wu; Xintian Wu; Xi Li*; Jian Tian</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">718 <b>Recovering the Unbiased Scene Graphs from the Biased Ones<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Meng-Jiun Chiou*; Henghui Ding; Hanshu YAN; Changhu Wang; Roger Zimmermann; Jiashi Feng</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">719 <b>Cross-modal Consensus Network for Weakly Supervised Temporal Action Localization<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Fa-Ting Hong; Jia-Chang Feng; Dan Xu; Ying Shan; Wei-Shi Zheng*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">722 <b>Searching a Hierarchically Aggregated Fusion Architecture for Fast Multi-Modality Image Fusion<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Risheng Liu*; Zhu Liu; Jinyuan Liu; Xin Fan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">724 <b>SuperFront: From Low-resolution to High-resolution Frontal Face Synthesis<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yu Yin*; Joseph Robinson; Songyao Jiang; Yue Bai; Can Qin; Yun Fu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">736 <b>Learning Segment Similarity and Alignment in Large-Scale Content Based Video Retrieval<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Chen Jiang*; Kaiming Huang; Sifeng He; Xudong Yang; Wei Zhang; Xiaobo Zhang; Yuan Cheng; Lei Yang; Qing Wang; Furong Xu; Tan Pan; Wei Chu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">770 <b>Cut-Thumbnail: A Novel Data Augmentation for Convolutional Neural Network<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Tianshu Xie; Xuan Cheng; Xiaomin Wang; Minghui Liu; Jiali Deng; Tao Zhou; Ming Liu*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">793 <b>Diffusing the Liveness Cues for Face Anti-spoofing<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Sheng Li; Xun Zhu; Guorui Feng; Xinpeng Zhang*; Zhenxing Qian</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">800 <b>Co-Transport for Class-Incremental Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Da-Wei Zhou*; Han-Jia Ye; De-Chuan Zhan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">802 <b>Skeleton-Contrastive 3D Action Representation Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Fida Mohammad Thoker*; Hazel Doughty; Cees Snoek</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">806 <b>Fast-forwarding, Rewinding, and Path Exploration in Interactive Branched Video Streaming<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Albin Vogel; Erik Kronberg; Niklas Carlsson*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">819 <b>Multiview Detection with Shadow Transformer (and View-Coherent Data Augmentation)<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yunzhong Hou*; Liang Zheng</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">820 <b>Domain Generalization via Feature Variation Decorrelation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Chang Liu*; Lichen Wang; Kai Li; Yun Fu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">829 <b>Occlusion-aware Bi-directional Guided Network for Light Field Salient Object Detection<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Dong Jing*; Shuo Zhang; Runmin Cong; Youfang Lin</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">831 <b>One-Stage Visual Grounding via Semantic-Aware Feature Filter<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jiabo Ye; Xin Lin*; Liang He; Dingbang Li; Qin Chen</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">840 <b>Few-Shot Multi-Agent Perception<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Chenyou Fan*; Junjie Hu; Jianwei Huang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">847 <b>SI3DP: Source Identification Challenges and Benchmark for Consumer-Level 3D Printer Forensics<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Bo Seok Shim; You Seung Shin; Seong wook Park; Jong-Uk Hou*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">850 <b>Exploring Sequence Feature Alignment for Domain Adaptive Detection Transformers<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Wen Wang; Yang Cao*; Jing Zhang; Fengxiang He; Zheng-Jun Zha; Yonggang Wen; Dacheng Tao</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">853 <b>Towards Realistic Visual Dubbing with Heterogeneous Sources<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Tianyi Xie*; Liucheng Liao; Cheng Bi; Benlai Tang; Xiang Yin; Jianfei Yang; Mingjie Wang; Jiali Yao; Yang Zhang; Zejun Ma</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">859 <b>Deep Self-Supervised t-SNE for Multi-modal Subspace Clustering<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Qianqian Wang*; Wei Xia; Zhiqiang Tao; Quanxue Gao; Xiaochun Cao</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">890 <b>Multimodal Video Summarization via Time-Aware Transformers<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xindi Shang*; Zehuan Yuan; Anran Wang; Changhu Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">892 <b>State-aware Video Procedural Captioning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Taichi Nishimura*; Atsushi Hashimoto; Yoshitaka Ushiku; Hirotaka Kameko; Shinsuke Mori</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">896 <b>AMSS-Net: Audio Manipulation on User-Specified Sources with Textual Queries<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Woosung Choi*; Minseok Kim; Marco Martnez Ramrez; Jaehwa Chung; Soonyoung Jung</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">904 <b>Fully Functional Image Manipulation Using Scene Graphs in A Bounding-Box Free Way<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Sitong Su*; Lianli Gao; Junchen Zhu; Jie Shao; Jingkuan Song</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">908 <b>Multi-Level Counterfactual Contrast for Visual Commonsense Reasoning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xi Zhang*; Feifei Zhang; Changsheng Xu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">916 <b>Data-Free Ensemble Knowledge Distillation for Privacy-conscious Multimedia Model Compression<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhiwei Hao*; Yong Luo; Han Hu; Jianping An; Yonggang Wen</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">926 <b>SM-SGE: A Self-Supervised Multi-Scale Skeleton Graph Encoding Framework for Person Re-Identification<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Haocong Rao*; Xiping Hu; Jun Cheng; Bin Hu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">940 <b>Video Transformer for Deepfake Detection with Incremental Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Sohail Ahmed Khan; Hang Dai*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">950 <b>Chinese Character Inpainting with Contextual Semantic Constraints<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jiahao Wang; Gang Pan*; Di Sun; Jiawan Zhang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">960 <b>Curriculum-Based Meta-learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Ji Zhang; Jingkuan Song*; Yazhou Yao; Lianli Gao</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">967 <b>Ego-Deliver: A Large-Scale Dataset for Egocentric Video Analysis<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Haonan Qiu; Pan He; Shuchun Liu; weiyuan shao; Feiyun Zhang; Jiajun Wang; Liang He; Feng Wang*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">975 <b>Adversarial Pixel Masking: A Defense against Physical Attacks for Pre-trained Object Detectors<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Bing-Han Chiang; Chi-Shen Chan; Shan-Hung Wu*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">987 <b>Knowledge-Supervised Learning: Knowledge Consensus Constraints for Person Re-Identification<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Li Wang; Baoyu Fan*; Zhenhua Guo; Yaqian Zhao; Runze Zhang; Rengang Li; Weifeng Gong; Endong Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">991 <b>View-normalized Skeleton Generation for Action Recognition<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Qingzhe Pan; Zhifu Zhao; Xuemei Xie*; Jianan Li; Yuhan Cao; Guangming Shi</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">995 <b>Learning Hierarchical Embedding for Video Instance Segmentation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zheyun Qin; Xiankai Lu*; Xiushan Nie; Xiantong Zhen; Yilong Yin</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1013 <b>Text as Neural Operator:Image Manipulation by Text Instruction<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Tianhao Zhang*; Hung-Yu Tseng; Lu Jiang; Weilong Yang; Honglak Lee; Irfan Essa</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1017 <b>DSANet: Dynamic Segment Aggregation Network for Video-Level Representation Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Wenhao Wu*; Yuxiang Zhao; Yanwu Xu; Xiao Tan; Dongliang He; Zhikang Zou; Jin Ye; Yingying Li; Mingde Yao; Dong Zichao; Yifeng Shi</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1029 <b>Structured Text Understanding with Multi-Modal Transformers<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yulin Li*; Yuxi Qian; Yuechen Yu; Xiameng Qin; Chengquan Zhang; Yan Liu; Kun Yao; Junyu Han; Jingtuo Liu; Errui Ding</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1034 <b>Local Graph Convolutional Networks for Cross-Modal Hashing<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yudong Chen*; Sen Wang; Jianglin Lu; Zhi Chen; Zheng Zhang; Zi Huang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1040 <b>Metric Learning for Anti-Compression Facial Forgery Detection<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Cao Shenhao; Qin Zou*; Xiuqing Mao; Dengpan Ye; Zhongyuan Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1042 <b>ASFM-Net: Asymmetrical Siamese Feature Matching Network for Point Completion<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yaqi Xia*; Yan Xia; Wei Li; Song Rui; Kailang Cao; Uwe M Stilla</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1049 <b>Capsule-based Object Tracking with Natural Language Specification<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Ding Ma*; XiangQian Wu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1084 <b>Faster-PPN: Towards Real-Time Semantic Segmentation with Dual Mutual Learning for Ultra-High Resolution Images<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Bicheng Dai; Kaisheng Wu; Tong Wu; Kai Li; Yanyun Qu*; Yuan Xie; Yun Fu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1104 <b>Distributed Attention for Grounded Image Captioning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Nenglun Chen*; Xingjia Pan; Runnan Chen; Lei Yang; Zhiwen Lin; Yuqiang Ren; Haolei Yuan; Xiaowei Guo; Feiyue Huang; Wenping Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1112 <b>Multi-initialization Optimization Network for Accurate 3D Human Pose and Shape Estimation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhiwei Liu*; Xiangyu Zhu; Lu Yang; Xiang Yan; Ming Tang; Zhen Lei; Guibo Zhu; Xuetao Feng; Yan Wang; Jinqiao Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1121 <b>Feedback Network for Mutually Boosted Stereo Image Super-Resolution and Disparity Estimation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Qinyan Dai; Juncheng Li; Qiaosi Yi; Faming Fang*; Guixu Zhang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1124 <b>Merging Multiple Template Matching Predictions in Intra Coding with Attentive Convolutional Neural Network<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Qijun Wang*; GuoDong Zheng</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1136 <b>Camera-Agnostic Person Re-Identification via Adversarial Disentangling Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Hao Ni*; Jingkuan Song; Xiaosu Zhu; Feng Zheng; Lianli Gao</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1138 <b>Learning to Understand Traffic Signs<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yunfei Guo*; Wei Feng; Fei yin; Tao Xue; Shuqi Mei; Cheng-Lin Liu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1142 <b>R-GAN: Exploring Human-like Way for Reasonable Text-to-Image Synthesis via Generative Adversarial Networks<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yanyuan Qiao*; Qi Chen; Chaorui Deng; Ning Ding; Yuankai Qi; Mingkui Tan; Xincheng Ren; Qi Wu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1168 <b>Cross-modality Discrepant Interaction Network for RGB-D Salient Object Detection<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Chen Zhang; Runmin Cong*; Qinwei Lin; Lin Ma; Feng Li; Yao Zhao; Sam Kwong</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1187 <b>Deconfounded and Explainable Interactive Vision-Language Retrieval of Complex Scenes<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Junda Wu; Tong Yu; Shuai Li*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1197 <b>Long Short-term Convolutional Transformer for No-Reference Video Quality Assessment<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Junyong You*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1205 <b>Automatic Channel Pruning with Hyper-parameter Search and Dynamic Masking<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Baopu Li*; Yanwen Fan; Zhihong Pan; Yuchen Bian; Gang Zhang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1215 <b>SVHAN: Sequential View Based Hierarchical Attention Network for 3D Shape Recognition<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yue Zhao; Weizhi Nie*; An-An Liu; Zan Gao; Yu-ting Su</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1218 <b>ASFD: Automatic and Scalable Face Detector<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jian Li*; Bin Zhang; Yabiao Wang; Ying Tai; Zhenyu Zhang; Chengjie Wang; Jilin Li; Xiaoming Huang; Yili Xia</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1223 <b>BridgeNet: A Joint Learning Network of Depth Map Super-Resolution and Monocular Depth Estimation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Qi Tang; Runmin Cong*; Ronghui Sheng; Lingzhi He; Dan Zhang; Yao Zhao; Sam Kwong</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1224 <b>LSTC: Boosting Atomic Action Detection with Long-Short-Term Context<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yuxi Li*; Boshen Zhang; Jian Li; Yabiao Wang; Weiyao Lin; Chengjie Wang; Jilin Li; Feiyue Huang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1229 <b>UACANet: Uncertainty Augmented Context Attention for Polyp Semgnetaion<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Taehun Kim*; Hyemin Lee; Daijin Kim</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1234 <b>Weight Evolution: Improving Deep Neural Networks Training through Evolving Inferior Weight Values<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhenquan Lin; Kailing Guo*; Xiaofen Xing; Xiangmin Xu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1241 <b>Coarse to Fine: Domain Adaptive Crowd Counting via Adversarial Scoring Network<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhikang Zou*; Xiaoye Qu; Pan Zhou; Shuangjie Xu; Xiaoqing Ye; Wenhao Wu; Jin Ye</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1259 <b>Towards Adversarial Patch Analysis and Certified Defense against Crowd Counting<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Qiming Wu*; Zhikang Zou; Pan Zhou; Xiaoqing Ye; Binghui Wang; Ang Li</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1265 <b>Conceptual and Syntactical Cross-modal Alignment with Cross-level Consistency for Image-Text Matching<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Pengpeng Zeng; Lianli Gao; Xinyu Lyu; Shuaiqi Jing; Jingkuan Song*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1270 <b>SSPU-Net: Self-Supervised Point Cloud Upsampling via Differentiable Rendering<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">YiFan Zhao; Le Hui; Jin Xie*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1275 <b>VmAP: A Fair Metric for Video Object Detection<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Anupam Sobti*; Vaibhav Mavi; M Balakrishnan; Chetan Arora</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1277 <b>Source Data-free Unsupervised Domain Adaptation for Semantic Segmentation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Mucong Ye; Jing Zhang*; Jinpeng Ouyang; Yuan Ding</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1280 <b>Yes, Attention Is All You Need, for Exemplar based Colorization<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Wang Yin*; Peng Lu; Zhaoran Zhao; Xujun Peng</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1283 <b>Heuristic Depth Estimation via Recurrent Prediction with Confidence-Aware Loss<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jiehua Zhang*; Liang Li; Chenggang Yan; Yaoqi Sun; Tao Shen; Jiyong Zhang; Zhan Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1291 <b>Unsupervised Cross-Modal Distillation for Thermal Infrared Tracking<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jingxian Sun; Lichao Zhang; Yufei Zha*; Abel Gonzalez-Garcia; Peng Zhang; Wei Huang; Yanning Zhang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1302 <b>ABPNet: Adaptive Background Modeling for Generalized Few Shot Segmentation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Kaiqi Dong*; Wei Yang; Zhenbo Xu; Liusheng Huang; Zhidong Yu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1319 <b>Towards Reasoning Ability in Scene Text Visual Question Answering<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Qingqing Wang*; Liqiang Xiao; Yue Lu; Yaohui Jin; Hao He</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1322 <b>Multi-caption Text-to-Face Synthesis: Dataset and Algorithm<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jianxin Sun*; Qi Li; Weining Wang; Jian Zhao; Zhenan Sun</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1323 <b>Multimodal Compatibility Modeling via Exploring the Consistent and Complementary Correlations<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Weili Guan*; Haokun Wen; Xuemeng Song; Chung-Hsing Yeh; Xiaojun Chang; Liqiang Nie</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1325 <b>CDD: Multi-view Subspace Clustering via Cross-view Diversity Detection<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Shudong Huang; Ivor Tsang; Zenglin Xu ; Jiancheng Lv; Quan-Hui Liu*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1330 <b>Learning Spatio-temporal Representation by Channel Aliasing Video Perception<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">YiQi Lin; Jinpeng Wang; Manlin Zhang; Andy J Ma*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1334 <b>Efficient Sparse Attacks on Videos using Reinforcement Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Huanqian Yan; Xingxing Wei*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1336 <b>AdvHash: Set-to-set Targeted Attack on Deep Hashing with One Single Adversarial Patch<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Shengshan Hu*; Yechao Zhang; Xiaogeng Liu; Leo Yu Zhang; Minghui Li; Hai Jin</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1343 <b>TransRefer3D: Entity-and-Relation Aware Transformer for Fine-Grained 3D Visual Grounding<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Dailan He*; Yusheng Zhao; Junyu Luo; Tianrui Hui; Shaofei Huang; Aixi Zhang; Si Liu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1344 <b>Single Image 3D Object Estimation with Primitive Graph Networks<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Qian He*; Desen Zhou; Bo Wan; Xuming He</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1346 <b>Boosting Mobile CNN Inference through Semantic Memory<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yun Li*; Chen Zhang; Shihao Han; Li Lyna Zhang; Baoqun Yin; Yunxin Liu; Mengwei Xu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1358 <b>Knowing When to Quit: Selective Cascaded Regression with Patch Attention for Real-Time Face Alignment<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Gil Shapira*; Noga Levy; Ishay Goldin; Roy J Jevnisek</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1371 <b>End-to-end Boundary Exploration for Weakly-supervised Semantic Segmentation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jianjun Chen*; Shancheng Fang; Hongtao Xie; Zheng-Jun Zha; Yue Hu; Jianlong Tan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1374 <b>SFE-Net: EEG-based emotion recognition with symmetrical spatial feature extraction<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xiangwen Deng; Junlin Zhu; Shangming Yang*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1379 <b>Bridging the Gap between Low-Light Scenes: Bilevel Learning for Fast Adaptation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Dian Jin; Long Ma; Risheng Liu*; Xin Fan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1391 <b>Handling Difficult Labels for Multi-label Image Classification via Uncertainty Distillation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Liangchen Song*; Jialian Wu; Ming Yang; Qian Zhang; Yuan Li; Junsong Yuan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1393 <b>Perception-Oriented Stereo Image Super-Resolution<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Chenxi Ma; Bo Yan*; Weimin Tan; Xuhao Jiang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1403 <b>ReLLIE: Deep Reinforcement Learning for Customized Low-Light Image Enhancement<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Rongkai Zhang*; Lanqing Guo; Siyu Huang; Bihan Wen</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1408 <b>Intrinsic Temporal Regularization for High-resolution Human Video Synthesis<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Lingbo Yang*; Zhanning Gao; Siwei Ma; Wen Gao</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1420 <b>A2W: Context-Aware Recommendation System for Mobile Augmented Reality Web Browser<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Kit Yung Lam*; Lik Hang Lee; Pan Hui</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1439 <b>Cross-modal Self-Supervised Learning for Lip Reading: When Contrastive Learning meets Adversarial Training<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Changchong Sheng; Matti Pietikinen; Qi Tian; Li Liu*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1450 <b>OsGG-Net: One-step Graph Generation Network for Unbiased Head Pose Estimation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Shentong Mo; Miao Xin*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1452 <b>Multi-Modal Multi-Instance Learning for Retinal Disease Recognition<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xirong Li*; Yang Zhou; Jie Wang; Hailan Lin; Jianchun Zhao; Dayong Ding; Weihong Yu; Youxin Chen</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1456 <b>Locally Adaptive Structure and Texture Similarity for Image Quality Assessment<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Keyan Ding*; Yi Liu; Xueyi Zou; Shiqi Wang; Kede Ma</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1458 <b>CALLip: Lipreading using Contrastive and Attribute Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yiyang Huang; Xuefeng Liang*; Chaowei Fang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1466 <b>Cross-Modal Recipe Embeddings by Disentangling Recipe Contents and Dish Styles<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yu Sugiyama; Keiji Yanai*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1473 <b>TDI TextSpotter: Taking Data Imbalance into Account in Scene Text Spotting<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yu Zhou*; Hongtao Xie; Shancheng Fang; Jing Wang; Zheng-Jun Zha; Yongdong Zhang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1477 <b>Position-Augmented Transformers with Entity-Aligned Mesh for TextVQA<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xuanyu Zhang*; Ching Yang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1479 <b>Learning Contextual Transformer Network for Image Inpainting<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Ye Deng*; Siqi Hui; Sanping Zhou; Deyu Meng; Jinjun Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1482 <b>Milliseconds Color Stippling<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Lei Ma; Jian Shi*; Yanyun Chen</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1485 <b>AFD-Net: Adaptive Fully-Dual Network for Few-Shot Object Detection<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Longyao Liu; Bo Ma*; Yulin Zhang; Xin Yi; Haozhi Li</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1487 <b>Missing Data Imputation for Solar Yield Prediction using Temporal Multi-Modal Variational Auto-Encoder<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Meng Shen*; Huaizheng Zhang; Yixin Cao; Fan Yang; Yonggang Wen</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1488 <b>Understanding Chinese Video and Language via Contrastive Multimodal Pre-Training<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Chenyi Lei*; Shixian Luo; Yong Liu; Wanggui He; Jiamang Wang; Guoxin Wang; Haihong Tang; Chunyan Miao; Houqiang Li</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1489 <b>DehazeFlow: Multi-scale Conditional Flow Network for Single Image Dehazing<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">HongYu Li*; Jia Li; Dong Zhao; Long Xu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1493 <b>GCM-Net: Towards Effective Global Context Modeling for Image Inpainting<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zheng Huan; Zhao Zhang*; Yang Wang; Zheng Zhang; Mingliang Xu; Yi Yang; Meng Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1496 <b>Embracing the Dark Knowledge: Domain Generalization Using Regularized Knowledge Distillation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yufei Wang*; Haoliang Li; Lap-Pui Chau; Alex Kot</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1499 <b>Cluster and Scatter: A Multi-grained Active Semi-supervised Learning Framework for Scalable Person Re-identification<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Bingyu Hu*; Zheng-Jun Zha; Jiawei Liu; Xierong Zhu; Hongtao Xie</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1516 <b>Dual Graph Convolutional Networks with Transformer and Curriculum Learning for Image Captioning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xinzhi Dong; Chengjiang Long; Wenju Xu; Chunxia Xiao*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1528 <b>Build Your Own Bundle - A Neural Combinatorial Optimization Method<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Qilin Deng*; Kai Wang; Minghao Zhao; Runze Wu; Yu Ding; Zhene Zou; Yue Shang; Jianrong Tao; Changjie Fan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1534 <b>Unsupervised Image Deraining: Optimization Model Driven Deep CNN<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Changfeng Yu; Yi Chang; Yi Li; Xi-Le Zhao; Luxin Yan*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1539 <b>An EM Framework for Online Class Incremental Semantic Segmentation with Dynamic Sampling<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Shipeng Yan*; Jiale Zhou; Jiangwei Xie; Songyang Zhang; Xuming He</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1548 <b>I2V-GAN: Unpaired Infrared-to-Visible Video Translation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Shuang Li*; Bingfeng Han; Zhenjie Yu; Chi Harold Liu; Kai Chen; Shuigen Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1549 <b>Implicit feedbacks are not always favorable: Iterative Relabeled One-Class Collaborative Filtering against Noisy Interactions<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zitai Wang*; Qianqian Xu; Zhiyong Yang; Xiaochun Cao; Qingming Huang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1555 <b>InsPose: Instance-Aware Networks for Single-Stage Multi-Person Pose Estimation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Dahu Shi*; Xing Wei; Xiaodong Yu; Wenming Tan; Ye Ren; Shiliang Pu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1566 <b>Implicit Feature Refinement for Instance Segmentation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Lufan Ma*; Tiancai Wang; Bin Dong; Jiangpeng Yan; Xiu Li; Xiangyu Zhang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1598 <b>Question-controlled Text-aware Image Captioning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Anwen Hu*; Shizhe Chen; Qin Jin</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1599 <b>Style-Aware Image Recommendation for Social Media Marketing<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yiwei Zhang*; Toshihiko Yamasaki</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1609 <b>WePerson: Learning a Generalized Re-identification Model from All-weather Virtual Data<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">He Li; Mang Ye*; Bo Du</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1622 <b>Polar Ray: A Single-stage Angle-free Detector for Oriented Object Detection in Aerial Images<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Shuai Liu; Lu Zhang; Shuai Hao; Huchuan Lu*; You He</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1623 <b>Self-Contrastive Learning with Hard Negative Sampling for Self-supervised Point Cloud Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Bi'an Du; Xiang Gao; Wei Hu*; Xin Li</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1626 <b>Generally Boosting Few-Shot Learning with HandCrafted Features<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yi Zhang; Sheng Huang*; Fengtao Zhou</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1640 <b>ROECS: A Robust Semi-direct Pipeline Towards Online Extrinsics Correction of the Surround-view System<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Tianjun Zhang; Nlong Zhao; Ying Shen; Xuan Shao; Lin Zhang*; Yicong Zhou</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1643 <b>Pseudo Graph Convolutional Network for Vehicle ReID<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Wen Qian*; Zhiqun He; Silong Peng; Chen Chen; Wei Wu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1658 <b>Towards Fast and High-Quality Sign Language Production<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Wencan Huang*; Wenwen Pan; Zhou Zhao; Qi Tian</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1660 <b>Effective De-identification Generative Adversarial Network for Face Anonymization<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhenzhong Kuang*; Huigui Liu; Jun Yu; Aikui Tian; Lei Wang; Jianping Fan; Noboru Babaguchi</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1666 <b>Cross-modal Retrieval and Synthesis (X-MRS): Closing the modality gap in shared subspace learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Ricardo Guerrero *; Hai X Pham; Vladimir Pavlovic</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1669 <b>When Face Completion Meets Irregular Holes: An Attributes Guided Deep Inpainting Network<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jie Xiao; Dandan Zhan; Haoran Qi; Zhi Jin*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1691 <b>Non-Linear Fusion for Self-Paced Multi-View Clustering<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zongmo Huang; Yazhou Ren*; Xiaorong Pu; Lifang He</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1694 <b>Counterfactual Debiasing Inference for Compositional Action Recognition<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Pengzhan Sun*; Bo Wu; Xunsong Li; Wen Li; Lixin Duan; Chuang Gan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1696 <b>STST: Spatial-Temporal Specialized Transformer for Skeleton-based Action Recognition<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yuhan Zhang*; Bo Wu; Wen Li; Lixin Duan; Chuang Gan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1700 <b>Exploring Gradient Flow Based Saliency for DNN Model Compression<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xinyu Liu; Baopu Li*; Zhen Chen; Yixuan Yuan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1701 <b>An adaptive iterative inpainting method with more information exploration<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Shengjie Chen*; Zhenhua Guo; Bo Yuan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1702 <b>Assisting News Media Editors with Cohesive Visual Storylines<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Gonalo Marcelino; David Semedo; Andre Mourao; Saverio Blasi; Joao Magalhaes*; Marta Mrak</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1713 <b>MM-Flow: Multi-modal Flow Network for Point Cloud Completion<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yiqiang Zhao; Yiyao Zhou*; Rui Chen; Bin Hu; Arxidin Akbar</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1722 <b>Long-tailed Distribution Adaptation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhiliang Peng; Wei Huang; Zonghao Guo; Xiaosong Zhang; Jianbin Jiao; Qixiang Ye*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1727 <b>Lesion-Inspired Denoising Network: Connecting Medical Image Denoising and Lesion Detection<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Kecheng Chen*; Kun Long; Yazhou Ren; Jiayu Sun; Xiaorong Pu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1736 <b>Domain Adaptive Semantic Segmentation without Source Data<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Fuming You; Jingjing Li*; Lei Zhu; Zhi Chen; Zi Huang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1738 <b>Cross-modal Joint Prediction and Alignment for Composed Query Image Retrieval<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yuchen Yang*; Min Wang; Wengang Zhou; Houqiang Li</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1743 <b>JDMAN: Joint Discriminative and Mutual Adaptation Networks for Cross-Domain Facial Expression Recognition<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yingjian Li; Yingnan Gao; Bingzhi Chen; Zheng Zhang; Lei Zhu; Guangming Lu*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1746 <b>Improving Weakly Supervised Object Localization via Causal Intervention<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Feifei Shao; Yawei Luo*; Li Zhang; Lu Ye; Siliang Tang; Yi Yang; Jun Xiao</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1754 <b>Imbalanced Source-free Domain Adaptation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Li Xinhao; Jingjing Li*; Lei Zhu; Guoqing Wang; Zi Huang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1757 <b>Learning Transferrable and Interpretable Representations for Domain Generalization<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhekai Du; Jingjing Li*; Ke Lu; Lei Zhu; Zi Huang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1768 <b>WAS-VTON: Warping Architecture Search for Virtual Try-on Network<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhenyu Xie; Xujie Zhang; Fuwei Zhao; Haoye Dong; Michael C. Kampffmeyer; Haonan Yan; Xiaodan Liang*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1777 <b>DFR-Net: A Novel Multi-Task Learning Network for Real-Time Multi-Instrument Segmentation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yan-Jie Zhou*; Shiqi Liu; Xiaoliang Xie; Zengguang Hou</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1779 <b>From Superficial to Deep: Language Bias driven Curriculum Learning for Visual Question Answering<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Mingrui Lao; Yanming Guo*; Yu Liu; Wei Chen; Nan Pu; Michael S Lew</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1780 <b>Pairwise Emotional Relationship Recognition in Drama Videos: Dataset and Benchmark<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xun Gao; Yin Zhao*; Jie Zhang; LongJun Cai</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1789 <b>Block Popularity Prediction for Multimedia Storage Systems Using Spacial-Temporal-Sequential Neural Networks<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yingying Cheng*; Fan Zhang; Gang Hu; Yiwen Wang; Hanhui Yang; Gong Zhang; Zhuo Cheng</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1790 <b>Transferrable Contrastive Learning for Visual Domain Adaptation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yang Chen; Yingwei Pan*; Yu Wang; Ting Yao; Xinmei Tian; Tao Mei</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1799 <b>Weighted Gaussian Loss based Hamming Hashing<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Rong-Cheng Tu*; Xian-Ling Mao; Cihang Kong; Zihang Shao; Ze-Lin Li; Wei Wei; Heyan Huang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1800 <b>Domain-Aware SE Network for Sketch-based Image Retrieval with Multiplicative Euclidean Margin Softmax<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Peng Lu; Gao Huang; Hangyu Lin; Wenming Yang*; Guodong Guo; Yanwei Fu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1803 <b>FTAFace: Context-enhanced Face Detector with Fine-grained Task Attention<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Deyu Wang*; Dongchao Wen; Wei Tao; Lingxiao Yin; Tse-Wei Chen; Tadayuki Ito; Kinya Osa; Masami Kato</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1820 <b>Identity-aware Graph Memory Network for Action Detection<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">JingCheng Ni*; Jie Qin; Di Huang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1822 <b>Improving Robustness and Accuracy via Relative Information Encoding in 3D Human Pose Estimation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Wenkang Shan*; Haopeng Lu; Shanshe Wang; Xinfeng Zhang; Wen Gao</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1824 <b>Deep Neural Network Retrieval<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Nan Zhong; Zhenxing Qian; Xinpeng Zhang*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1827 <b>Adversarial Learning with Mask Reconstruction for Text-Guided Image Inpainting<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xingcai Wu*; Yucheng Xie; Jiaqi Zeng; Zhenguo Yang; Yi Yu; Qing Li; Wenyin Liu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1835 <b>Spatiotemporal Inconsistency Learning for DeepFake Video Detection<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhihao Gu*; Yang Chen; Taiping Yao; Shouhong Ding; Jilin Li; Feiyue Huang; Lizhuang Ma</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1838 <b>VeloCity: Using Voice Assistants for Cyclists to Provide Traffic Reports<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Gian-Luca Savino*; Jesse Moraes Braga; Johannes Schoening</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1845 <b>Edit Like A Designer: Modeling Design Workflows for Unaligned Fashion Editing<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Qiyu Dai; Shuai Yang; Wenjing Wang; Wei Xiang; Jiaying Liu*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1848 <b>Privacy-Preserving Portrait Matting<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jizhizi Li; Sihan Ma; Jing Zhang*; Dacheng Tao</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1858 <b>A Transformer based Approach for Image Manipulation Chain Detection<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jiaxiang You; Yuanman Li*; Jiantao Zhou; Zhongyun Hua; Weiwei Sun; Xia Li</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1862 <b>HANet: Hierarchical Alignment Networks for Video-Text Retrieval<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Peng Wu*; Xiangteng He; Mingqian Tang; Yiliang Lv; Jing Liu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1866 <b>Scalable Multi-view Subspace Clustering with Unified Anchors<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Mengjing Sun; Pei Zhang*; Siwei Wang; Sihang Zhou; Wenxuan Tu; Xinwang Liu; En Zhu; Changjian Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1868 <b>PRNet: A Progressive Recovery Network for Revealing Perceptually Encrypted Images<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Tao Xiang; Ying Yang; Shangwei Guo*; Hangcheng Liu; Hantao Liu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1881 <b>FakeTagger: Robust Safeguards against DeepFake Dissemination via Provenance Tracking<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Run Wang*; Felix Juefei-Xu; Meng Luo; Yang Liu; Lina Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1885 <b>Discriminative Latent Semantic Graph for Video Captioning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yang Bai*; Junyan Wang; Yang Long; Bingzhang Hu; Yang Song; Maurice Pagnucco; Yu Guan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1888 <b>From Image to Imuge: Immunized Image Generation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Qichao Ying; Zhenxing Qian*; Hang Zhou; Haisheng Xu; Xinpeng Zhang; Siyi Li</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1894 <b>Wisdom of (Binned) Crowds: A Bayesian Stratification Paradigm for Crowd Counting<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Sravya V Shivapuja; Mansi P Khamkar; Divij Bajaj; Ganesh Ramakrishnan; Ravi Kiran Sarvadevabhatla*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1896 <b>Demystifying Commercial Video Conferencing Applications<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Insoo Lee*; Jinsung Lee; Kyunghan Lee; Dirk Grunwald; Sangtae Ha</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1918 <b>LightFEC: Network Adaptive FEC with A Lightweight Deep-Learning Approach<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Han Hu*; Sheng Cheng; Xinggong Zhang; Zongming Guo</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1935 <b>SOGAN: 3D-Aware Shadow and Occlusion Robust GAN for Makeup Transfer<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yueming Lyu; Jing Dong*; Bo Peng; Wei Wang; Tieniu Tan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1954 <b>Semantic Scalable Image Compression with Cross-Layer Priors<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Hanyue Tu*; Li Li; Wengang Zhou; Houqiang Li</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1966 <b>Cascade Cross-modal Attention Network for Video Actor and Action Segmentation from a Sentence<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Weidong Chen*; Guorong Li; Xinfeng Zhang; Hongyang Yu; Shuhui Wang; Qingming Huang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1971 <b>Extracting Useful Knowledge form Noisy Web Images via Data Purification for Fine-Grained Recognition<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Chuanyi Zhang; Yazhou Yao*; Xing Xu; Jie Shao; Jingkuan Song; Zechao Li; Zhenmin Tang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1974 <b>Complementary Factorization towards Outfit Compatibility Modeling<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Tianyu Su*; Xuemeng Song; Na Zheng; Weili Guan; Yan Li; Liqiang Nie</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1983 <b>Open Set Face Anti-Spoofing in Unseen Attacks<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xin Dong; Hao Liu*; Weiwei Cai; Pengyuan Lv; Zekuan Yu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1990 <b>Interventional Video Relation Detection<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yicong Li*; Xun Yang; Xindi Shang; Tat-Seng Chua</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">1997 <b>CanvasEmb: Learning Layout Representationwith Large-scale Pre-training for Graphic Design<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yuxi Xie*; Danqing Huang; Jinpeng Wang; Chin-Yew Lin</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2005 <b>Augmenting TV Shows via Uncalibrated Camera Small Motion Tracking in Dynamic Scene<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yizhen Lao*; Jie Yang; Xinying Wang; Jianxin Lin; Cao Yu; Song shien</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2007 <b>SimulSLT: End-to-End Simultaneous Sign Language Translation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Aoxiong Yin; Zhou Zhao*; Jinglin Liu; Weike Jin; Meng Zhang; Xingshan Zeng; Xiaofei He</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2009 <b>Mask and Predict: Multi-step Reasoning for Scene Graph Generation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Hongshuo Tian; Ning Xu*; An-An Liu; Chenggang Yan; Zhendong Mao; Quan Zhang; Yongdong Zhang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2022 <b>Heterogeneous Face Recognition with Attention-guided Feature Disentangling<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Shanming Yang; Xiao Yang*; Yi Lin; Peng Cheng; Yi Zhang; Jianwei Zhang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2026 <b>Exploring the Quality of GAN Generated Images for Person Re-Identification<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yiqi Jiang*; Weihua Chen; Xiuyu Sun; Xiaoyu Shi; Fan Wang; Hao Li</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2033 <b>Multi-view Clustering via Deep Matrix Factorization and Partition Alignment<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Chen Zhang*; Siwei Wang; Jiyuan Liu; Sihang Zhou; Pei Zhang; Xinwang Liu; En Zhu; Changwang Zhang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2035 <b>Video Similarity and Alignment Learning on Partial Video Copy Detection<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhen Han*; Xiangteng He; Mingqian Tang; Yiliang Lv</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2048 <b>No-Reference Video Quality Assessment with Heterogeneous Knowledge Ensemble<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jinjian Wu*; Yongxu Liu; Leida Li; Weisheng Dong; Guangming Shi</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2061 <b>Seeing is Believing? Effects of Visualization on Smart Device Privacy Perceptions<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Carlos Bermejo Fernandez*; Petteri Nurmi; Pan Hui</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2062 <b>MHFC: Multi-Head Feature Collaboration for Few-Shot Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Shuai Shao*; Lei Xing; Yan Wang; Rui Xu; Chunyan Zhao; Yanjiang Wang; Baodi Liu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2072 <b>Vision-guided Music Source Separation via a Fine-grained Cycle-Separation Network<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Ma Shuo; Yanli Ji*; Xing Xu; Xiaofeng Zhu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2086 <b>GLM-Net : Global and Local Motion Estimation via Task-Oriented Encoder-Decoder Structure<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yuchen Yang; Ye Xiang; Shuaicheng Liu; Megvii; Lifang Wu*; Boxuan Zhao; Bing Zeng</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2089 <b>Sensor-Augmented Egocentric-Video Captioning with Dynamic Modal-Attention<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Katsuyuki Nakamura*; Hiroki Ohashi; Mitsuhiro Okada</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2091 <b>Cross Modal Compression: Towards Human-comprehensible Semantic Compression<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jiguo Li*; Chuanmin Jia; Xinfeng Zhang; Siwei Ma; Wen Gao</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2100 <b>RAMS-Trans: Recurrent Attention Multi-scale Transformer for Fine-grained Image Recognition<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yunqing Hu; Xuan Jin; Yin Zhang*; Haiwen Hong; Jingfeng Zhang; Yuan He; hui xue</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2103 <b>Memory-Augmented Deep Unfolding Network for Compressive Sensing<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jiechong Song; Bin Chen; Jian Zhang*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2105 <b>Underwater Species Detection using Channel Sharpening Attention<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Lihao Jiang; Yi Wang*; Qi Jia; Shengwei Xu; Yu Liu; Xin Fan; Haojie Li; Risheng Liu; Xinwei Xue; Ruili Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2110 <b>Self-Supervised Pre-training on the Target Domain for Cross-Domain Person Re-identification<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Junyin Zhang; Yongxin Ge*; Xinqian Gu; Boyu Hua; Tao Xiang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2115 <b>Exploring Graph-Structured Semantics for Cross-Modal Retrieval<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Lei Zhang; Leiting Chen; Chuan Zhou*; Fan Yang; Xin Li</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2130 <b>Text is NOT Enough: Integrating Visual Impressions into Open-domain Dialogue Generation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Lei Shen*; Haolan Zhan; Xin Shen; Yonghao Song; Xiaofang Zhao</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2143 <b>Quality Assessment of End-to-End Learned Image Compression: The Benchmark and Objective Measure<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yang Li*; Shiqi Wang; Xinfeng Zhang; Shanshe Wang; Siwei Ma; Yue Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2144 <b>A Statistical Approach to Mining Semantic Similarity for Deep Unsupervised Hashing<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xiao Luo; Daqing Wu; Zeyu Ma; Chong Chen*; Minghua Deng; Jianqiang Huang; Xian-Sheng Hua</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2153 <b>BAM: Bilateral Activation Mechanism for Image Fusion<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zi-Rong Jin; Liang-Jian Deng*; Tian-Jing Zhang; Xiaoxu Jin</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2161 <b>Hallucinating Statistical Moment and Subspace Descriptors for Action Recognition<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Lei Wang*; Piotr Koniusz</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2168 <b>Learning Multi-Granular Spatio-Temporal Graph Network for Skeleton-based Action Recognition<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Tailin Chen*; Desen Zhou; Jian Wang; Shidong Wang; Yu Guan; Xuming He; Errui Ding</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2179 <b>ION: Instance-level Object Navigation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Weijie Li*; Xinhang Song; Yubing Bai; Sixian Zhang; Shuqiang Jiang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2187 <b>Skeleton-Aware Neural Sign Language Translation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Shiwei Gan; Yafeng Yin*; Zhiwei Jiang; Lei Xie; Sanglu Lu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2210 <b>Fingerspelling Recognition in the Wild with Fixed-Query based Visual Attention<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Srinivas Kruthiventi S S*; George Jose; Nitya Tandon; Rajesh Biswal; AASHISH KUMAR</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2219 <b>Deep Human Dynamics Prior<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Qiongjie Cui*; Huaijiang Sun; Yue Kong; Xiaoning Sun</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2231 <b>Exploiting Invariance of Mining Facial Landmarks<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jiangming Shi; Zixian Gao; Hao Liu*; Zekuan Yu; Fengjun Li</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2240 <b>Joint Implicit Image Function for Guided Depth Super-Resolution<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jiaxiang Tang*; Xiaokang Chen; Gang Zeng</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2260 <b>Transformer-based Feature Reconstruction Network for Robust Multimodal Sentiment Analysis<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Ziqi Yuan; Wei Li; Hua Xu*; Wenmeng Yu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2287 <b>Self-feature learning: an efficient deep lightweight network for image super-resolution<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jun Xiao*; Qian Ye; Rui Zhao; Kin-Man Lam; Kao Wan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2298 <b>DAWN: Dynamic Adversarial Watermarking of Neural Networks<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Sebastian Szyller*; Buse Gul Atli; Samuel Marchal; N. Asokan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2325 <b>Visible Watermark Removal via Self-calibrated Localization and Background Refinement<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jing Liang; Li Niu*; Fengjun Guo; Teng Long; Liqing Zhang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2332 <b>Learning to Decode Contextual Information for Efficient Contour Detection<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Ruoxi Deng*; Shengjun Liu; Jinxin Wang; Huibing Wang; Hanli Zhao; Xiaoqin Zhang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2336 <b>Fast, High-Quality Hierarchical Depth-Map Super-Resolution<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yiguo Qiao*; Licheng Jiao; Wenbin Li; Christian Richardt; Darren Cosker</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2363 <b>TsFPS: An Accurate and Flexible 6DoF Tracking System with Fiducial Platonic Solids<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Nan Xiang*; Xiaosong Yang; Jian J Zhang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2367 <b>Consistency-Constancy Bi-Knowledge Learning for Pedestrian Detection in Night Surveillance<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xiao Wang; Zheng Wang*; Wu Liu; Xin Xu; Jing Chen; Chia-Wen Lin</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2371 <b>SSconv: Explicit Spectral-to-Spatial Convolution for Pansharpening<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yudong Wang; Liang-Jian Deng*; Tian-Jing Zhang; Xiao Wu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2374 <b>TriTransNet:RGB-D salient object detection with a triplet transformer embedding network<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhengyi Liu*; Yuan Wang; Zhengzheng Tu; Yun Xiao; Bin Tang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2377 <b>Learning Sample-Specific Policies for Sequential Image Augmentation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Pu Li*; Xiaobai Liu; Xiaohui Xie</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2378 <b>Image Quality Caption with Attentive and Recurrent Semantic Attractor Network<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Wen Yang; Jinjian Wu*; Leida Li; Weisheng Dong; Guangming Shi</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2390 <b>Triangle-Reward Reinforcement Learning: A Visual-Linguistic Semantic Alignment for Image Captioning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Weizhi Nie; Jiesi Li; Ning Xu*; An-An Liu; Xuanya Li; Yongdong Zhang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2429 <b>Stacked Semantically-Guided Learning for Image De-distortion<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Huiyuan Fu*; Changhao Tian; Xin Wang; Huadong Ma</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2438 <b>Focal and Composed Vision-semantic Modeling for Visual Question Answering<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yudong Han*; Yangyang Guo; Jianhua Yin; Meng Liu; Yupeng Hu; Liqiang Nie</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2444 <b>Pose-Guided Feature Learning with Knowledge Distillation for Occluded Person Re-Identification<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Kecheng Zheng*; Cuiling Lan; Wenjun Zeng; Jiawei Liu; Zhizheng Zhang; Zheng-Jun Zha</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2450 <b>Multiple Objects-Aware Visual Question Generation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jiayuan Xie; Yi Cai*; Qingbao Huang; Tao Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2454 <b>VASTile: Viewport Aware Scalable 360-Degree Video Frame Tiling.<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Chamara Manoj Madarasingha Kattadige*; Kanchana Thilakarathna</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2462 <b>Delving into Deep Image Prior for Adversarial Defense: A Novel Reconstruction-based Defense Framework<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Li Ding*; Yongwei Wang; Xin Ding; Kaiwen Yuan; Ping Wang; Hua Huang; Z. Jane Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2464 <b>Fine-Grained Language Identification in Scene Text Images<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yongrui Li*; Shilian Wu; Jun Yu; Zengfu Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2472 <b>CARE: CloudIfied Android OSes on the Cloud Rendering<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Dongjie Tang; Cathy Bao; Yong Yao; Chao Xie; Qiming Shi; Marc Mao; Randy Xu; Linsheng Li; Mohammad Reza R Haghighat; Zhengwei Qi*; Haibing Guan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2474 <b>Context-Aware Selective Label Smoothing for Calibrating Sequence Recognition Models<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Shuangping Huang; Yu Luo*; Zhenzhou Zhuang; Jin-Gang Yu; Mengchao He; Yongpan Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2476 <b>Image Search with Text Feedback by Deep Hierarchical Attention Mutual Information Maximization<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Chunbin Gu*; Jiajun Bu; Zhen Zhang; Zhi Yu; Dongfang Ma; Wei Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2481 <b>Pairwise VLAD Interaction Network for Video Question Answering<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Hui Wang; Dan Guo*; Xian-Sheng Hua; Meng Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2487 <b>Attention-guided Temporally Coherent Video Object Matting<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yunke Zhang; Chi Wang; Miaomiao Cui; Peiran Ren; Xuansong Xie; Xian-Sheng Hua; Hujun Bao; Qixing Huang; Weiwei Xu*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2492 <b>Disentangling Hate in Online Memes<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Roy Ka-Wei Lee*; Rui Cao; Fan Ziqing; Jing Jiang; Wen Haw Chong</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2496 <b>Robust Real-World Image Super-Resolution against Adversarial Attacks<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jiutao Yue; Haofeng Li; Pengxu Wei; Guanbin Li*; Liang Lin</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2500 <b>Towards Robust Deep Hiding Under Non-Differentiable Distortions for Practical Blind Watermarking<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Chaoning Zhang*; Adil Karjauv; Philipp Benz; In So Kweon</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2502 <b>Bottom-Up and Bidirectional Alignment for Referring Expression Comprehension<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Liuwu Li; Yuqi Bu; Yi Cai*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2527 <b>SalS-GAN: Spatially-Adaptive Latent Space in StyleGAN for Real Image Embedding<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Lingyun Zhang; Xiuxiu Bai*; Yao Gao</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2531 <b>Structured Multi-modal Feature Embedding and Alignment for Image-Sentence Retrieval<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xuri Ge*; Fuhai Chen; Joemon Jose; Zhilong Ji; Zhongqin Wu; Xiao Liu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2537 <b>Keyframe Extraction from Motion Capture Sequences with Graph based Deep Reinforcement Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Clinton A Mo*; Kun Hu; Shaohui Mei; Zebin Chen; Zhiyong Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2547 <b>Dense Contrastive Visual-Linguistic Pretraining<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Lei Shi*; Kai Shuang; Shijie Geng; Peng Gao; Zuohui Fu; Gerard de Melo; Yunpeng Chen; Sen Su</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2550 <b>Hybrid Reasoning Network for Video-based Commonsense Captioning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Weijiang Yu*; Jian Liang; Lei Ji; Lu Li; Yuejian Fang; Nong Xiao; Nan Duan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2563 <b>Learning Regularizer for Monocular Depth Estimation with Adversarial Guidance<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Guibao Shen; Yingkui Zhang; Jialu Li; Mingqiang Wei; Qiong Wang*; Guangyong Chen; Pheng-Ann Heng</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2568 <b>Pixel-wise Graph Attention Networks For Person Re-identification<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Wenyu Zhang; Qing Ding*; Jian Hu; Yi Ma; Mingzhe Lu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2571 <b>Neighbor-Vote: Improving Monocular 3D Object Detection through Neighbor Distance Voting<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xiaomeng Chu; Jiajun Deng; Yao Li; Zhenxun Yuan; Yanyong Zhang*; Jianmin Ji; Yu Zhang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2574 <b>Remember and Reuse: Cross-Task Blind Image Quality Assessment via Relevance-aware Incremental Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Rui Ma; Hanxiao Luo; Qingbo Wu*; King Ngi Ngan; Hongliang Li; Fanman Meng; LinFeng Xu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2584 <b>MSO: Multi-Feature Space Joint Optimization Network for RGB-Infrared Person Re-Identification<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yajun Gao; Tengfei Liang; Yi Jin*; Xiaoyan Gu; Wu Liu; Yidong Li; Congyan Lang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2591 <b>Point Cloud Projection and Multi-Scale Feature Fusion Network Based Blind Quality Assessment for Colored Point Clouds<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Wen-xu Tao; Gang-yi Jiang*; Zhi-di Jiang; Mei Yu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2592 <b>Multi-branch Channel-wise Enhancement Network for Fine-grained Visual Recognition<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Guangjun Li*; Yongxiong Wang; Fengting Zhu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2626 <b>General Approximate Cross Validation for Model Selection: Supervised, Semi-supervised and Pairwise Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Bowei Zhu; Yong Liu*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2637 <b>Progressive and Selective Fusion Network for High Dynamic Range Imaging<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Qian Ye*; Jun Xiao; Kin-Man Lam; Takayuki Okatani</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2639 <b>Multimodal Relation Extraction with Efficient Graph Alignment<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Changmeng Zheng; Junhao Feng; Ze Fu; Yi Cai*; Qing Li; Tao Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2640 <b>Legitimate Adversarial Patches: Evading Human Eyes and Detection Models in the Physical World<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jia Tan*; Nan Ji; Haidong Xie; Xueshuang Xiang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2645 <b>Unsupervised Vehicle search in the Wild: A New Benchmark<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xian Zhong; Shilei Zhao; Xiao Wang*; Kui Jiang; Wenxuan Liu; Wenxin Huang; Zheng Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2650 <b>Meta-FDMixup: Cross-Domain Few-Shot Learning Guided by Labeled Target Data<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yuqian Fu*; Yanwei Fu; Yu-Gang Jiang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2651 <b>Target-guided Adaptive Base Class Reweighting for Few-Shot Learning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jiliang Yan; Deming Zhai; Junjun Jiang; Xianming Liu*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2671 <b> Dynamic Reasoning Network for Few-shot Semantic Segmentation <b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yunzhi Zhuge; Chunhua Shen*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2680 <b>Heterogeneous Feature Fusion and Cross-modal Alignment for Composed Image Retrieval<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Gangjian Zhang; Shikui Wei*; Huaxin Pang; Yao Zhao</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2702 <b>Similar Scenes arouse Similar Emotions: Parallel Data Augmentation for Stylized Image Captioning<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Guodun Li; Yuchen Zhai; Zehao Lin; Yin Zhang*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2705 <b>Trajectory is not Enough: Hidden Following Detection<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Danni Xu*; Ruimin Hu; Zixiang Xiong; Zheng Wang; Linbo Luo; Dengshi Li</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2707 <b>Contrastive Learning for Cold-Start Recommendation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yinwei Wei*; Xiang Wang; Qi Li; Liqiang Nie; Yan Li; Xuanping Li; Tat-Seng Chua</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2708 <b>CG-GAN: Class-Attribute Guided Generative Adversarial Network for Old Photo Restoration<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jixin Liu*; Rui Chen; Shipeng An; Heng Zhang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2726 <b>Get The Best of the Three Worlds: Real-Time Neural Image Compression in a Non-GPU Environment<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zekun Zheng*; Xiaodong Wang; Xinye Lin; Shaohe Lv</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2728 <b>Visual Language Based Succinct Zero-Shot Object Detection<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Ye Zheng*; Xi Huang; Li Cui</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2733 <b>GAMnet: Robust Feature Matching via Graph Adversarial-Matching Network<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Bo Jiang*; Pengfei Sun; Ziyan Zhang; Jin Tang; Bin Luo</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2734 <b>MCCN: Multimodal Coordinated Clustering Network for Large-Scale Cross-modal Retrieval<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhixiong Zeng*; Ying Sun; Wenji Mao</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2742 <b>AFEC: Adaptive Feature Extraction Modules For Learned Image Compression<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yi Ma*; Yongqi Zhai; Jiayu Yang; Chunhui Yang; Ronggang Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2768 <b>How Video Super-Resolution and Frame Interpolation Mutually Benefit<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Chengcheng Zhou*; Zongqing Lu; Linge Li; Qiangyu Yan; Jing-Hao Xue</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2772 <b>FOCAS: Practical Video Super Resolution using Foveated Rendering<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Lingdong Wang*; Mohammad Hajiesmaili; Ramesh K. Sitaraman</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2778 <b>Adaptive Affinity Loss and Erroneous Pseudo-Label Refinement for Weakly Supervised Semantic Segmentation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xiangrong Zhang*; Zelin Peng; Peng Zhu; Tianyang Zhang; Chen Li; Huiyu Zhou; Licheng Jiao</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2782 <b>Relationship-Preserving Knowledge Distillation for Zero-Shot Sketch Based Image Retrieval<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jialin Tian; Xing Xu*; Zheng Wang; Fumin Shen; Xin Liu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2818 <b>Partially fake it till you make it: mixing real and fake thermal images for improved object detection<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Francesco Bongini; Lorenzo Berlincioni; Marco Bertini*; Alberto Del Bimbo</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2826 <b>CDP: Towards Optimal Filter Pruning via Class-wise Discriminative Power<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Tianshuo Xu; Yuhang Wu; Xiawu Zheng; Teng Xi; Gang Zhang; Errui Ding; Fei Chao*; Rongrong Ji</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2834 <b>Face Hallucination via Split-Attention in Split-Attention Network<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Tao Lu; Yuanzhi Wang*; Yanduo Zhang; Yu Wang; Liu Wei; Zhongyuan Wang; Junjun Jiang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2858 <b>Fake Gradient: A Security and Privacy Protection Framework for DNN-based Image Classification<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xianglong Feng; Yi Xie; Mengmei Ye; Zhongze Tang; Bo Yuan; Sheng Wei*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2877 <b>Integrating Semantic and Temporal Relationships in Facial Action Unit Detection<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhihua Li*; Xiang Deng; Xiaotian Li; Lijun Yin</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2887 <b>Sparse to Dense Depth Completion using a Generative Adversarial Network with Intelligent Sampling Strategies<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Md Fahim Faysal Khan*; Nelson D Troncoso Aldas; Abhishek Kumar; Siddharth Advani; Vijaykrishnan Narayanan</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2898 <b>How does Color Constancy Affect Target Recognition and Instance Segmentation ?<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Siyan Xue; Shaobing Gao*; Minjie Tan; Zhen He; Liangtian He</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2911 <b>Convolutional Transformer based Dual Discriminator Generative Adversarial Networks for Video Anomaly Detection<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Xinyang Feng; Dongjin Song*; Yuncong Chen; Zhengzhang Chen; Jingchao Ni; Haifeng Chen</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2921 <b>Salient Error Detection based Refinement for Wide-baseline Image Interpolation<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yuan Chang*; Yisong Chen; Guoping Wang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2936 <b>A Multi-Domain Adaptive Graph Convolutional Network for EEG-based Emotion Recognition<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Rui Li*; Yiting Wang; Bao-Liang Lu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2939 <b>Interpolation Variable Rate Image Compression<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhenhong Sun; Zhiyu Tan; Xiuyu Sun*; Fangyi Zhang; Yichen Qian; Dongyang Li; Hao Li</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2959 <b>Armor: A Benchmark for Testing the Effectiveness of Artificial Music Objective Evaluation Method<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Songhe Wang*; Zheng Bao; Jingtong E</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2974 <b>DRDF: Determining the Importance of Different Multimodal Information with Dual-Router Dynamic Framework<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Haiwen Hong; Xuan Jin; Yin Zhang*; Yunqing Hu; Jingfeng Zhang; Yuan He; hui xue</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">2977 <b>CoCo-BERT: Improving Video-Language Pre-training with Contrastive Cross-modal Matching and Denoising<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jianjie Luo; Yehao Li; Yingwei Pan*; Ting Yao; Hongyang Chao; Tao Mei</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">3000 <b>DLA-Net for FG-SBIR: Dynamic Local Aligned Network for Fine-Grained Sketch-Based Image Retrieval<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jiaqing Xu*; Haifeng Sun; Qi Qi; Jingyu Wang; Ce Ge; Lejian Zhang; Jianxin Liao</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">3004 <b>Pareto Optimality for Fairness-constrained Collaborative Filtering<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Qianxiu Hao*; Qianqian Xu; Zhiyong Yang; Qingming Huang</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">3005 <b>Decoupled IoU Regression for Object Detection<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Yan Gao*; Qimeng Wang; Xu Tang; Haochen Wang; Fei Ding; Jing Li; Yao Hu</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">3018 <b>RCNet: Reverse Feature Pyramid and Cross-scale Shift Network for Object Detection<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Zhuofan Zong; Qianggang Cao; Biao Leng*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">3026 <b>Recursive Fusion and Deformable Spatiotemporal Attention for Video Compression Artifact Reduction<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Minyi Zhao; Yi Xu; Shuigeng Zhou*</p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperTitle el-col el-col-24"><p data-v-fe7dd554="">3084 <b>JokerGAN: Memory-Efficient Model for Handwritten Text Generation with Text Line Awareness<b></b></b></p></div></div></div><div data-v-fe7dd554=""><div data-v-fe7dd554="" class="el-row"><div data-v-fe7dd554="" class="paperAuthor el-col el-col-24"><p data-v-fe7dd554="">Jan Zdenek*; Hideki Nakayama</p></div></div></div></div>